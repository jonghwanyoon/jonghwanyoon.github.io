<!DOCTYPE html>
<html lang=ko>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="naver-site-verification" content="29892ce904ec6054137e0bb84d2f2197d1eb56f4" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>ML | Logistic Regression과 Stochastic Gradient Descent | jonghwanyoon</title><meta name="robots" content="noindex">
  <meta name="description" content="Logistic Regression Logistic regression은 선형 방정식을 이용한 분류 알고리즘입니다. Linear regression 처럼 변수의 상과관계를 표현하는 방정식을 만들지만, 각각의 클래스에 속할 확률을 예측하는 것이 목적입니다. 그래서 선형 방정식을 학습한다는 것은 이전에 보여드린 Linear Regression과 같지만, 결">
<meta property="og:type" content="article">
<meta property="og:title" content="ML | Logistic Regression과 Stochastic Gradient Descent">
<meta property="og:url" content="https://jonghwanyoon.github.io/2023/01/24/hongong_ml_03/index.html">
<meta property="og:site_name" content="Jonghwan Yoon">
<meta property="og:description" content="Logistic Regression Logistic regression은 선형 방정식을 이용한 분류 알고리즘입니다. Linear regression 처럼 변수의 상과관계를 표현하는 방정식을 만들지만, 각각의 클래스에 속할 확률을 예측하는 것이 목적입니다. 그래서 선형 방정식을 학습한다는 것은 이전에 보여드린 Linear Regression과 같지만, 결">
<meta property="og:locale">
<meta property="article:published_time" content="2023-01-24T03:00:03.000Z">
<meta property="article:modified_time" content="2023-07-03T12:13:37.557Z">
<meta property="article:author" content="Jonghwan Yoon">
<meta property="article:tag" content="혼공학습단">
<meta property="article:tag" content="혼공">
<meta property="article:tag" content="혼공머신">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="https://jonghwanyoon.github.io/2023/01/24/hongong_ml_03/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Jonghwan Yoon" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://jonghwanyoon.github.io/" target="_blank">
          <img class="img-circle img-rotate" src="/images/blue_whale.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">jonghwanyoon</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Seoul, South Korea</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">Home</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">Archives</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">Categories</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">Tags</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">About</span>
          </a>
        </li>
        
      </ul>
      <!-- 
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/jonghwanyoon" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.linkedin.com/in/jonghwan-yoon" target="_blank" title="Linkedin" data-toggle=tooltip data-placement=top><i class="icon icon-linkedin"></i></a></li>
        
    </ul>
 -->
    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      
  <div class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%EC%B1%85/">책</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%EC%B1%85/%ED%98%BC%EC%9E%90-%EA%B3%B5%EB%B6%80%ED%95%98%EB%8A%94-%EC%8B%9C%EB%A6%AC%EC%A6%88/">혼자 공부하는 시리즈</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/">혼공학습단</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Hexo/" style="font-size: 13.5px;">Hexo</a> <a href="/tags/%EB%A6%AC%EB%B7%B0/" style="font-size: 13px;">리뷰</a> <a href="/tags/%EC%B1%85/" style="font-size: 13.5px;">책</a> <a href="/tags/%EC%BD%94%EB%94%A9-%EC%9D%B8%ED%84%B0%EB%B7%B0/" style="font-size: 13px;">코딩 인터뷰</a> <a href="/tags/%ED%98%BC%EA%B3%B5/" style="font-size: 14px;">혼공</a> <a href="/tags/%ED%98%BC%EA%B3%B5C/" style="font-size: 13.5px;">혼공C</a> <a href="/tags/%ED%98%BC%EA%B3%B5S/" style="font-size: 13px;">혼공S</a> <a href="/tags/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0/" style="font-size: 13px;">혼공머신</a> <a href="/tags/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/" style="font-size: 14px;">혼공학습단</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/">혼공학습단</a>
              </p>
              <p class="item-title">
                <a href="/2023/08/13/hongong-c-05/" class="title">혼자 공부하는 C - 배열</a>
              </p>
              <p class="item-date">
                <time datetime="2023-08-13T10:40:50.000Z" itemprop="datePublished">2023-08-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%EC%B1%85/">책</a>
              </p>
              <p class="item-title">
                <a href="/2023/07/13/book-cracking-the-coding-interview/" class="title">(책) 코딩 인터뷰 완전분석</a>
              </p>
              <p class="item-date">
                <time datetime="2023-07-13T11:50:00.000Z" itemprop="datePublished">2023-07-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Hexo/">Hexo</a>
              </p>
              <p class="item-title">
                <a href="/2023/07/03/you-must-add-nojekyll-in-hexo/" class="title">Hexo에서 Github pages의 deploy fail시 .nojekyll을 추가하자.</a>
              </p>
              <p class="item-date">
                <time datetime="2023-07-03T13:29:13.000Z" itemprop="datePublished">2023-07-03</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%EC%B1%85/">책</a><i class="icon icon-angle-right"></i><a class="category-link" href="/categories/%EC%B1%85/%ED%98%BC%EC%9E%90-%EA%B3%B5%EB%B6%80%ED%95%98%EB%8A%94-%EC%8B%9C%EB%A6%AC%EC%A6%88/">혼자 공부하는 시리즈</a>
              </p>
              <p class="item-title">
                <a href="/2023/07/03/hongong-10th-summary/" class="title">(책) 혼자 공부하는 C언어 정리</a>
              </p>
              <p class="item-date">
                <time datetime="2023-07-02T15:00:00.000Z" itemprop="datePublished">2023-07-03</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%EC%B1%85/">책</a><i class="icon icon-angle-right"></i><a class="category-link" href="/categories/%EC%B1%85/%ED%98%BC%EC%9E%90-%EA%B3%B5%EB%B6%80%ED%95%98%EB%8A%94-%EC%8B%9C%EB%A6%AC%EC%A6%88/">혼자 공부하는 시리즈</a>
              </p>
              <p class="item-title">
                <a href="/2023/03/09/hongong-9th-summary/" class="title">(책) 혼자 공부하는 ML/DL, SQL 정리</a>
              </p>
              <p class="item-date">
                <time datetime="2023-03-09T11:54:43.000Z" itemprop="datePublished">2023-03-09</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-hongong_ml_03" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      ML | Logistic Regression과 Stochastic Gradient Descent
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2023/01/24/hongong_ml_03/" class="article-date">
	  <time datetime="2023-01-24T03:00:03.000Z" itemprop="datePublished">2023-01-24</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/">혼공학습단</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%ED%98%BC%EA%B3%B5/" rel="tag">혼공</a>, <a class="article-tag-link-link" href="/tags/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0/" rel="tag">혼공머신</a>, <a class="article-tag-link-link" href="/tags/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/" rel="tag">혼공학습단</a>
  </span>


        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2023/01/24/hongong_ml_03/#comments" class="article-comment-link">Comments</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <p><br><br></p>
<h1 id="logistic-regression"><a class="markdownIt-Anchor" href="#logistic-regression"></a> Logistic Regression</h1>
<p>Logistic regression은 선형 방정식을 이용한 <strong>분류</strong> 알고리즘입니다. Linear regression 처럼 변수의 상과관계를 표현하는 방정식을 만들지만, 각각의 클래스에 속할 확률을 예측하는 것이 목적입니다.</p>
<p>그래서 선형 방정식을 학습한다는 것은 이전에 보여드린 Linear Regression과 같지만, 결과는 0 ~ 1사이의 확률로 표현하여 합니다.</p>
<p>예를 들어, 아래와 같은 생선의 종류, 무게, 길이, 대각선, 높이, 두께 데이터가 있고, 생선의 종류의 확률을 예측하는 모델을 만든다고 해보겠습니다.</p>
<p><br><br></p>
<table>
<thead>
<tr>
<th>Species</th>
<th>Weight</th>
<th>Length</th>
<th>Diagonal</th>
<th>Height</th>
<th>Width</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bream</td>
<td>242</td>
<td>25.4</td>
<td>30</td>
<td>11.52</td>
<td>4.02</td>
</tr>
<tr>
<td>Bream</td>
<td>290</td>
<td>26.3</td>
<td>31.2</td>
<td>12.48</td>
<td>4.3056</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>Smelt</td>
<td>19.7</td>
<td>14.3</td>
<td>15.2</td>
<td>2.8728</td>
<td>2.0672</td>
</tr>
<tr>
<td>Smelt</td>
<td>19.9</td>
<td>15</td>
<td>16.2</td>
<td>2.9322</td>
<td>1.8792</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<p>선형 방정식은 다음 처럼 학습할 것입니다.</p>
<p>z = a * <code>Weight</code> + b * <code>Length</code> + c * <code>Diagonal</code> + d * <code>Height</code> + e * <code>Width</code> + f</p>
<p>a, b, c, d, e는 가중치를 나타내고, f는 절편입니다.</p>
<ul>
<li>z는 주어지는 생선 데이터에 따라 무엇이든 나올 수 있습니다.</li>
<li>하지만 확률을 예측해야 하므로, z에 따라 확률로 나타내야합니다.</li>
<li>이를 가능하게 해주는 것이 Sigmoid Function입니다. (Logistic Fuction이라고도 함)</li>
</ul>
<p><br><br></p>
<center>
<img width=150px src=https://wikimedia.org/api/rest_v1/media/math/render/svg/faaa0c014ae28ac67db5c49b3f3e8b08415a3f2b>
<br>
<p><font color=gray>Sigmoid Function<br></font></p>
</center>
<br>
<ul>
<li>z값이 커질 수록 1을, 작을수록 0을 나타냅니다.</li>
<li>z에 따라 표시하면 아래과 같습니다.</li>
</ul>
<iframe src="/html/hongong_ml_03_fig1.html" width="100%" height="400" frameborder="0" loading="lazy" allowfullscreen></iframe>
<p><br><br></p>
<p>scikit-learnd에서도 LogisticRegression으로 제공하고 있습니다. Binary logistic regression과 multinomial logistic regression을 할 수 있습니다.</p>
<p><br><br></p>
<h2 id="binary-classification"><a class="markdownIt-Anchor" href="#binary-classification"></a> Binary Classification</h2>
<p>Logistic regression을 이용해 2가지 클래스에 대한 분류를 진행해보겠습니다.</p>
<p><strong>1. 데이터 전처리</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 데이터 불러오기</span></span><br><span class="line">fish = pd.read_csv(<span class="string">&quot;https://bit.ly/fish_csv_data&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train, Test 나누기. 0번째 column이 타겟입니다.</span></span><br><span class="line">train_input, test_input, train_target, test_target = \</span><br><span class="line">    train_test_split(fish.iloc[:, <span class="number">1</span>:], fish.iloc[:, <span class="number">0</span>], random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardization</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">ss.fit(train_input)</span><br><span class="line">train_scaled = ss.transform(train_input)</span><br><span class="line">test_scaled = ss.transform(test_input)</span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p><strong>2. Bream과 Smelt의 이중분류 해보기</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터에서 Bream과 Smelt 데이터 분리 </span></span><br><span class="line">bream_smelt_indexes = (train_target == <span class="string">&quot;Bream&quot;</span>) | (train_target == <span class="string">&quot;Smelt&quot;</span>)</span><br><span class="line">train_bream_smelt = train_scaled[bream_smelt_indexes]</span><br><span class="line">target_bream_smelt = train_target[bream_smelt_indexes]</span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p><strong>3. LogisticRegression 사용해보기</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(train_bream_smelt, target_bream_smelt) <span class="comment"># 학습</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train 데이터의 앞 5개 만 예측해보기</span></span><br><span class="line"><span class="built_in">print</span>(lr.predict(train_bream_smelt[:<span class="number">5</span>])) <span class="comment"># 예측</span></span><br><span class="line"><span class="comment"># [&#x27;Bream&#x27;, &#x27;Smelt&#x27;, &#x27;Bream&#x27;, &#x27;Bream&#x27;, &#x27;Bream&#x27;]</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p><strong>학습</strong></p>
<p>linear regression과 마찬가지로 주어진 데이터의 변수들에 대한 선형 방정식을 학습합니다.</p>
<p>lr.coef_와 lr.intercept_ 에 다음처럼 가중치 (혹은 계수)와 절편이 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.coef_)</span><br><span class="line"><span class="comment"># [[-0.4037798 , -0.57620209, -0.66280298, -1.01290277, -0.73168947]]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lr.intercept_)</span><br><span class="line"><span class="comment"># [-2.16155132]</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p>lr.coef_의 결과는 입력된 train데이터에 따라 Weight, Length, Diagonal, Height , Width의 가중치입니다.</p>
<p>따라서 방정식은 다음과 같습니다.</p>
<blockquote>
<p>z = (-0.4037798) * Weight + (-0.57620209) * Length + (-0.66280298) * Diagonal + (-1.01290277) * Height + (-0.73168947) * Width + (-2.16155132)</p>
</blockquote>
<p><br><br></p>
<p><strong>예측</strong></p>
<p>train 데이터의 5개를 predict를 하였을 때, <code>'Bream', 'Smelt', 'Bream', 'Bream', 'Bream'</code>으로 나타났습니다.</p>
<p>예측한 확률을 <code>predict_proba</code>을 이용해 확인하면 다음과 같습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">lr.predict_proba(train_bream_smelt[:<span class="number">5</span>])</span><br><span class="line"><span class="comment"># array([[0.99759855, 0.00240145],</span></span><br><span class="line"><span class="comment">#        [0.02735183, 0.97264817],</span></span><br><span class="line"><span class="comment">#        [0.99486072, 0.00513928],</span></span><br><span class="line"><span class="comment">#        [0.98584202, 0.01415798],</span></span><br><span class="line"><span class="comment">#        [0.99767269, 0.00232731]])</span></span><br><span class="line"></span><br><span class="line">lr.classes_</span><br><span class="line"><span class="comment"># array([&#x27;Bream&#x27;, &#x27;Smelt&#x27;], dtype=object)</span></span><br></pre></td></tr></table></figure>
<br>
<ul>
<li>predict_proba의 결과로 5개의 데이터셋에 대한 확률을 2개씩 내었는데, 주어진 <code>class</code>의 순서에 따라 확률을 표시합니다.</li>
<li>class 의 순서는 <code>lr.classes_</code>로 보면 첫 번째는 <code>Bream</code>, 2번 째는 <code>Smelt</code>입니다.</li>
<li>안에서 일어나는 일은, 위에 학습된 선형 방정식에 따른 z값을 얻고, sigmoid 함수를 통해 0~1 사이의 값으로 변환이 가능합니다.</li>
</ul>
<p><br><br></p>
<p><strong>z 값 계산</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">decisions = lr.decision_function(train_bream_smelt[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(decisions)</span><br><span class="line"><span class="comment"># [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<h4 id="sigmoid를-이용한-확률-계산"><a class="markdownIt-Anchor" href="#sigmoid를-이용한-확률-계산"></a> Sigmoid를 이용한 확률 계산</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> expit</span><br><span class="line"><span class="built_in">print</span>(expit(decisions))</span><br><span class="line"><span class="comment"># [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p>실습을 위해 선형방정식을 이용해 직접 계산해도 같은 값이 나옵니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_decision</span>(<span class="params">dataset</span>):</span><br><span class="line">    z_list = []</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">        z = <span class="built_in">float</span>(<span class="built_in">sum</span>((lr.coef_ * data)[<span class="number">0</span>]) + lr.intercept_)</span><br><span class="line">        z_list.append(z)</span><br><span class="line">    <span class="keyword">return</span> np.array(z_list)</span><br><span class="line"></span><br><span class="line">decisions = make_decision(train_bream_smelt[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(decisions)</span><br><span class="line"><span class="comment"># [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]</span></span><br><span class="line"><span class="built_in">print</span>(expit(decisions))</span><br><span class="line"><span class="comment"># [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<h2 id="다중-분류"><a class="markdownIt-Anchor" href="#다중-분류"></a> 다중 분류</h2>
<p>scikit learn의 LogisticRegression은 기본적으로 규제와 최대 반복수가 정해져 있습니다. 파라미터 기본값은 다음과 같습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.linear_model.LogisticRegression(</span><br><span class="line">    penalty=<span class="string">&#x27;l2&#x27;</span>,</span><br><span class="line">    *,</span><br><span class="line">    dual=<span class="literal">False</span>,</span><br><span class="line">    tol=<span class="number">0.0001</span>,</span><br><span class="line">    C=<span class="number">1.0</span>,</span><br><span class="line">    fit_intercept=<span class="literal">True</span>,</span><br><span class="line">    intercept_scaling=<span class="number">1</span>,</span><br><span class="line">    class_weight=<span class="literal">None</span>,</span><br><span class="line">    random_state=<span class="literal">None</span>,</span><br><span class="line">    solver=<span class="string">&#x27;lbfgs&#x27;</span>,</span><br><span class="line">    max_iter=<span class="number">100</span>,</span><br><span class="line">    multi_class=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">    verbose=<span class="number">0</span>,</span><br><span class="line">    warm_start=<span class="literal">False</span>,</span><br><span class="line">    n_jobs=<span class="literal">None</span>,</span><br><span class="line">    l1_ratio=<span class="literal">None</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>여기서 <code>C</code>는 <code>Inverse of regularization strength</code> 으로 릿지 회귀에서는 alpha가 커질 수록 규제가 강해졌지만, 여기서는 반대로 (inverse) 값이 작을 수록 규제가 커집니다.</p>
<p><code>max_iter</code>는 <code>Maximum number of iterations taken for the solvers to converge</code> 으로, 훈련을 하는 반복수입니다. 반복이 부족하다면 학습이 제대로 되지 않을 수 있고, 경고가 발생합니다.</p>
<p>자세한 설명은 <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">공식문서</a>에 있습니다.</p>
<p><br><br></p>
<h3 id="실습"><a class="markdownIt-Anchor" href="#실습"></a> 실습</h3>
<p>이번엔 여러개 클래스에 대해 예측을 해보겠습니다.</p>
<p>생선의 종류는 ‘Bream’ ‘Pike’ ‘Smelt’ ‘Perch’ ‘Parkki’ ‘Roach’ 'Whitefish’으로 7가지입니다.</p>
<br>
<table>
<thead>
<tr>
<th>Species</th>
<th>Weight</th>
<th>Length</th>
<th>Diagonal</th>
<th>Height</th>
<th>Width</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bream</td>
<td>242</td>
<td>25.4</td>
<td>30</td>
<td>11.52</td>
<td>4.02</td>
</tr>
<tr>
<td>Bream</td>
<td>290</td>
<td>26.3</td>
<td>31.2</td>
<td>12.48</td>
<td>4.3056</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>Smelt</td>
<td>19.7</td>
<td>14.3</td>
<td>15.2</td>
<td>2.8728</td>
<td>2.0672</td>
</tr>
<tr>
<td>Smelt</td>
<td>19.9</td>
<td>15</td>
<td>16.2</td>
<td>2.9322</td>
<td>1.8792</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<p>이번에는 규제를 완화하여 20으로 하고, max_iter는 기본값 100보다 충분히 주기 위해 1000으로 지정하겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(C = <span class="number">20</span>, max_iter = <span class="number">1000</span>)</span><br><span class="line">lr.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(lr.score(train_scaled, train_target))</span><br><span class="line"><span class="comment"># 0.9327731092436975</span></span><br><span class="line"><span class="built_in">print</span>(lr.score(test_scaled, test_target))</span><br><span class="line"><span class="comment"># 0.925</span></span><br></pre></td></tr></table></figure>
<p>이번엔 데이터셋에 주어진 생선의 종류를 모두 사용하여 학습하였습니다.<br>훈련 데이터셋과 테스트 데이터셋의 성능이 overfitting이나 underfitting없이 나타나는 것 같습니다.</p>
<br>
<p>이번에는 테스트셋 5개의 예측결과, 예측된 확률을 살펴보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.predict(test_scaled[:<span class="number">5</span>]))</span><br><span class="line"><span class="comment"># [&#x27;Perch&#x27; &#x27;Smelt&#x27; &#x27;Pike&#x27; &#x27;Roach&#x27; &#x27;Perch&#x27;]</span></span><br><span class="line"></span><br><span class="line">prob = lr.predict_proba(test_scaled[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">round</span>(prob, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># [[0.    0.014 0.841 0.    0.136 0.007 0.003]</span></span><br><span class="line"><span class="comment">#  [0.    0.003 0.044 0.    0.007 0.946 0.   ]</span></span><br><span class="line"><span class="comment">#  [0.    0.    0.034 0.935 0.015 0.016 0.   ]</span></span><br><span class="line"><span class="comment">#  [0.011 0.034 0.306 0.007 0.567 0.    0.076]</span></span><br><span class="line"><span class="comment">#  [0.    0.    0.904 0.002 0.089 0.002 0.001]]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lr.classes_)</span><br><span class="line"><span class="comment"># [&#x27;Bream&#x27; &#x27;Parkki&#x27; &#x27;Perch&#x27; &#x27;Pike&#x27; &#x27;Roach&#x27; &#x27;Smelt&#x27; &#x27;Whitefish&#x27;]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>predict_proba의 결과를 보면 예측할 클래스 7개에 대한 확률을 각각의 데이터에 대해 보여줍니다.</li>
<li>그리고 binary logistic regression에서 말씀드린 것 처럼, class의 순서에 따라 확률을 표시합니다.</li>
<li>예측된 방정식을 보면 다음과 같습니다.</li>
</ul>
<br>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.coef_.shape)</span><br><span class="line"><span class="built_in">print</span>(lr.coef_)</span><br><span class="line"><span class="comment"># (7, 5)</span></span><br><span class="line"><span class="comment"># [[-1.49001912 -1.02912113  2.59344919  7.70357975 -1.20070316]</span></span><br><span class="line"><span class="comment">#  [ 0.19618178 -2.01069031 -3.77976124  6.50491703 -1.99482274]</span></span><br><span class="line"><span class="comment">#  [ 3.56279889  6.34356697 -8.48970852 -5.75757441  3.79307052]</span></span><br><span class="line"><span class="comment">#  [-0.10458149  3.60319752  3.93067857 -3.61737459 -1.75069665]</span></span><br><span class="line"><span class="comment">#  [-1.4006158  -6.07503214  5.25969469 -0.87219731  1.86043717]</span></span><br><span class="line"><span class="comment">#  [-1.38526207  1.49214216  1.39225873 -5.67734316 -4.40097705]</span></span><br><span class="line"><span class="comment">#  [ 0.62149782 -2.32406307 -0.90661142  1.7159927   3.69369191]]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lr.intercept_.shape)</span><br><span class="line"><span class="built_in">print</span>(lr.intercept_)</span><br><span class="line"><span class="comment"># (7,)</span></span><br><span class="line"><span class="comment"># [-0.09205174 -0.2629083   3.25101298 -0.14742661  2.65498449 -6.78783562</span></span><br><span class="line"><span class="comment">#   1.38422481]</span></span><br></pre></td></tr></table></figure>
<br>
<p>7개의 class에 대한 5가지 feature 이므로 방정식의 크기 또한 다릅니다. 절편 또한 7개로 되어있습니다. multinomial logistic regression에서는 class마다 z값을 하나씩 계산합니다.</p>
<p>그러면 7개의 z값이 존재합니다. binary regression에서는 하나의 z값으로 두개를 분류했는데, 여기서는 어떻게 분류할까요?</p>
<p>scikit learn의 predict_proba의 설명을 보면 다음과 같습니다.</p>
<blockquote>
<p>The returned estimates for all classes are ordered by the label of classes.</p>
<p>For a multi_class problem, <strong>if multi_class is set to be “multinomial” the softmax function is used to find the predicted probability of each class</strong>. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.</p>
</blockquote>
<ul>
<li>multinomical의 경우에는 softmax 함수를 사용합니다.</li>
<li>softmax는 7개의 z값을 모두 합친 값이 1이 되도록 하여 확률로 변환합니다. 각각의 값에 모두 합친 값을 나누어 줍니다.</li>
</ul>
<p>이번에도 구해진 z값을 softmax를 이용해 계산해서 같은 값인지 확인해보겠습니다.</p>
<p>5개 데이터의 대한 7개 z값은 다음과 같습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">decision = lr.decision_function(test_scaled[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">round</span>(decision, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># [[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]</span></span><br><span class="line"><span class="comment">#  [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]</span></span><br><span class="line"><span class="comment">#  [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]</span></span><br><span class="line"><span class="comment">#  [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]</span></span><br><span class="line"><span class="comment">#  [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]</span></span><br></pre></td></tr></table></figure>
<p>scipy의 softmax함수를 이용해 계산했을 때, predict_proba의 결과와 같음을 볼 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> softmax</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">round</span>(softmax(decision, axis=<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line"><span class="comment"># [[0.    0.014 0.841 0.    0.136 0.007 0.003]</span></span><br><span class="line"><span class="comment">#  [0.    0.003 0.044 0.    0.007 0.946 0.   ]</span></span><br><span class="line"><span class="comment">#  [0.    0.    0.034 0.935 0.015 0.016 0.   ]</span></span><br><span class="line"><span class="comment">#  [0.011 0.034 0.306 0.007 0.567 0.    0.076]</span></span><br><span class="line"><span class="comment">#  [0.    0.    0.904 0.002 0.089 0.002 0.001]]</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p>여기서 중간점검을 한번 해보겠습니다.</p>
<p>Logistic regression에서 이진 분류의 확률을 출력하기 위해 사용하는 확률은 무엇일까요?</p>
<p><br><Br></p>
<p>답은 <strong>Sigmoid</strong>함수입니다. 이진 분류에서는 선형 방정식을 통해 계산된 z값을 두가지 클래스 중에 확정해야 하는데,</p>
<p>값이 작을수록 0을 나타내고 값이 클 수록 1을 나타내는 sigmoid 함수를 이용하면 클래스의 확률을 0, 1에 가깝게 예측할 수 있습니다. 0.5를 기준으로 높으면 1, 낮으면 0 으로 분류가 가능하므로 두가지 클래스에서 하나를 선택할 수 있는 확률이 구해집니다.</p>
<p><br><br></p>
<h1 id="stochastic-gradient-descent-sgd"><a class="markdownIt-Anchor" href="#stochastic-gradient-descent-sgd"></a> Stochastic Gradient Descent (SGD)</h1>
<p>지금까지의 방법은 모델을 선언하고, 학습시키고, 예측하는 방법이었습니다. 만약 학습할 데이터가 나중에도 계속해서 추가된다면 어떻게 할까요? 기존처럼 한다면 학습시킨 모델을 버리고 처음부터 학습시키게 되는데, 훈련 데이터가 어마어마하게 크다면 리소스가 많이 소모될 것입니다. 이럴 때는 점진적으로 학습하는 알고리즘을 사용할 수 있습니다. 대표적으로 사용되는 것은 SGD입니다.</p>
<ul>
<li>Stochastic이라는 말은 학습을 시작할 데이터를 랜덤하게 선택한다는 의미입니다.</li>
<li>Gradient라는 기울기를 통해 손실을 계산하는 것을 말합니다.</li>
<li>Gradient descent라는 것은 손실을 계산해서 낮은 쪽을 따라 내려가는 것을 의미입니다.</li>
</ul>
<p>정리하면 훈련 데이터에서 하나씩 데이터를 꺼내서 학습할 때 손실을 계산하여 낮은 쪽을 따라 학습하는 알고리즘 입니다.</p>
<p>SGD는 데이터를 하나씩 사용해서 손실를 줄이는 방향으로 학습합니다. 이를 반복하여 전체 데이터셋을 학습하는 것을 하나의 <strong>에포크 (epoch)</strong> 라고 합니다. 보통은 한번의 에포크가 아닌 수백번의 에포크를 거쳐야합니다. 왜냐하면 아주 조금씩 학습이 되기 때문입니다.</p>
<p><br><br></p>
<center>
<img width=800px src=https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png>
<font color=gray>
<p>출처: <a target="_blank" rel="noopener" href="https://www.jeremyjordan.me/nn-learning-rate/">https://www.jeremyjordan.me/nn-learning-rate/</a><br />
</font></center></p>
<p><br><br></p>
<p>데이터를 하나씩 꺼내서 학습시키고 다음 경사를 따라 갑니다. 조금씩 학습하는 이유는 여기에 있습니다. 조금씩 오차를 계산할 때마다 점점 낮아지므로 다음 단계도 낮을거라는 기대를 가지고 학습하기 때문에, 학습률이 높다면 도로 올라갈 수도 있습니다.</p>
<p><br><br></p>
<center>
<img width=800px src=https://www.mltut.com/wp-content/uploads/2020/04/Untitled-document-3.png
>
<font color=gray>
<p>출처: <a target="_blank" rel="noopener" href="https://www.mltut.com/wp-content/uploads/2020/04/Untitled-document-3.png">https://www.mltut.com/wp-content/uploads/2020/04/Untitled-document-3.png</a></p>
<p></font></center></p>
<p><br><br></p>
<p>또한 Stochastic (확률적)이라는 것은 랜덤하게 데이터를 뽑는 것이라고 말씀드렸는데, 이는 그림에서 나타나는 문제를 보완하기 위함입니다. 랜덤으로 하지 않는다면, 우리가 이상적으로 얻고자하는 Global Minimum이 아닌 처음 찾은 Local minimum이 진짜라고 착각하고 끝내게 될 수도 있습니다.</p>
<p>그리고 성능을 예측하려면 지표가 필요한데, 손실함수<sup>Loss Function</sup>가 손실이 어느정도인지 계산을 해줍니다.</p>
<ul>
<li>Regression에서는 Mean Squeared Error (MSE)나 Mean Absolute Error (MAE)를 사용할 수 있습니다.</li>
<li>Classification에서는 cross entropy loss function (logistic loss function)을 사용할 수 있습니다.</li>
</ul>
<p>SGD에 대해 더 궁금하시다면 다음 영상도 참고해주세요.</p>
<p><br><br></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/vMh0zPT0tLI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p><br><br></p>
<h2 id="실습-2"><a class="markdownIt-Anchor" href="#실습-2"></a> 실습</h2>
<ul>
<li>이번에도 classification 을 해보겠습니다.</li>
<li>scikit learn에서 SGDClassifier를 제공해줍니다.</li>
</ul>
<br>
<p><strong>데이터 불러오기 및 전처리</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 불러오기</span></span><br><span class="line">fish = pd.read_csv(<span class="string">&quot;https://bit.ly/fish_csv_data&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train, Test 나누기. 0번째 column이 타겟입니다.</span></span><br><span class="line">train_input, test_input, train_target, test_target = \</span><br><span class="line">    train_test_split(fish.iloc[:, <span class="number">1</span>:], fish.iloc[:, <span class="number">0</span>], random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardization</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">ss.fit(train_input)</span><br><span class="line">train_scaled = ss.transform(train_input)</span><br><span class="line">test_scaled = ss.transform(test_input)</span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p><strong>SGDClassifier 모델 사용해 예측점수 확인</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"></span><br><span class="line">sc = SGDClassifier(loss = <span class="string">&quot;log_loss&quot;</span>, max_iter = <span class="number">10</span>, random_state = <span class="number">42</span>)</span><br><span class="line">sc.fit(train_scaled, train_target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sc.score(train_scaled, train_target))</span><br><span class="line"><span class="comment"># 0.773109243697479</span></span><br><span class="line"><span class="built_in">print</span>(sc.score(test_scaled, test_target))</span><br><span class="line"><span class="comment"># 0.775</span></span><br></pre></td></tr></table></figure>
<p>train 데이터와 test 데이터의 점수가 낮습니다. 지금은 에포크를 10번으로 지정하여 출력한 결과입니다. 또한 아래의 경고메세지도 같이 나옵니다. 이는 모델이 충분히 수렴하지 않았다는 의미이고, 이럴 때에는 max_iter를 높여서 진행하는 것이 좋습니다.</p>
<blockquote>
<p>ConvergenceWarning:<br />
Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.mber of iteration reached before convergence. Consider increasing max_iter to improve the fit.</p>
</blockquote>
<p>SGD는 점진적 학습을 하는 알고리즘이라 말씀드렸는데, partial_fit으로 모델을 이어서 훈련할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 에포크 추가하여 학습</span></span><br><span class="line">sc.partial_fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(sc.score(train_scaled, train_target))</span><br><span class="line"><span class="comment"># 0.8151260504201681</span></span><br><span class="line"><span class="built_in">print</span>(sc.score(test_scaled, test_target))</span><br><span class="line"><span class="comment"># 0.8</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p>모델의 성능이 더 좋아지는 것을 확인할 수 있습니다. 그러면 많이 학습시키면 계속해서 좋아질까요?</p>
<br>
<p>약 300 에포크 정도를 학습시켜보고, 훈련 데이터셋 점수와 테스트 데이터셋 점수를 비교해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sc = SGDClassifier(loss = <span class="string">&quot;log_loss&quot;</span>, random_state = <span class="number">42</span>)</span><br><span class="line"><span class="comment"># sc.fit(train_scaled, train_target)</span></span><br><span class="line"></span><br><span class="line">train_score = []</span><br><span class="line">test_score = []</span><br><span class="line">classes = np.unique(train_target)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">300</span>):</span><br><span class="line">    sc.partial_fit(train_scaled, train_target, classes = classes)</span><br><span class="line">    train_score.append(sc.score(train_scaled, train_target))</span><br><span class="line">    test_score.append(sc.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p>데이터셋이 작아서 크게 차이는 없지만 아래 그림에서 확인해보면, 100 에포크 이후로 훈련 데이터셋과 테스트 데이터셋의 성능에 차이가 나타나는 것을 볼 수 있습니다. Overfitting이 나타나는 지점으로 보입니다.</p>
<iframe src="/html/hongong_ml_03_fig2.html" width="100%" height="500" frameborder="0" loading="lazy" allowfullscreen></iframe>
<p>따라서, 여기서는 100 에포크까지만 학습한 모델을 예측에 사용하는 것이 좋습니다</p>
<p>오늘은 Logistic Regression과 Stochastic Gradient Descent에 대해 알아보았습니다. 다음에는 트리 알고리즘, 교차검증, 앙상블에 대해 정리해보겠습니다.</p>
<p>읽어주셔서 감사합니다.👋</p>
<p><br><br><br><br></p>
<!-- flag of hidden posts -->
      
    </div>
    <div class="article-footer">
      <!-- <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://jonghwanyoon.github.io/2023/01/24/hongong_ml_03/" title="ML | Logistic Regression과 Stochastic Gradient Descent" target="_blank" rel="external">https://jonghwanyoon.github.io/2023/01/24/hongong_ml_03/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote> -->
<!-- 

<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://jonghwanyoon.github.io/" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/blue_whale.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://jonghwanyoon.github.io/" target="_blank"><span class="text-dark">jonghwanyoon</span><small class="ml-1x"></small></a></h3>
        <div></div>
      </div>
    </figure>
  </div>
</div>
 -->

    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      </div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/jonghwanyoon" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.linkedin.com/in/jonghwan-yoon" target="_blank" title="Linkedin" data-toggle=tooltip data-placement=top><i class="icon icon-linkedin"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        &copy; 2023 Jonghwan Yoon
        
        <div class="publishby">
        	<!-- Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>. -->
            Theme by <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
    <script defer>
    var disqus_config = function () {
        
            this.page.url = 'https://jonghwanyoon.github.io/2023/01/24/hongong_ml_03/';
        
        this.page.identifier = 'hongong_ml_03';
    };
    (function() { 
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'wg-yoon' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>






    <script defer type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-140384355-2', 'auto');
ga('send', 'pageview');

</script>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>