<!DOCTYPE html>
<html lang=ko>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="naver-site-verification" content="29892ce904ec6054137e0bb84d2f2197d1eb56f4" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>ML | Decision Tree, Cross validation, Ensemble | jonghwanyoon</title><meta name="robots" content="noindex">
  <meta name="description" content="decision tree, cross validation, ensemble에 대해 정리하는 글입니다.   Decision Tree Decision tree는 마치 순서도 처럼 예&#x2F;아니오 같은 이진으로 분류된 기준을 이어나가면서 분류를 하는 알고리즘 입니다. 회귀도 가능하지만 분류 위주로 정리되었습니다. Decision Tree의 장점은 해석하기 쉽다는데">
<meta property="og:type" content="article">
<meta property="og:title" content="ML | Decision Tree, Cross validation, Ensemble">
<meta property="og:url" content="https://jonghwanyoon.github.io/2023/01/31/hongong-ml-04-md/index.html">
<meta property="og:site_name" content="Jonghwan Yoon">
<meta property="og:description" content="decision tree, cross validation, ensemble에 대해 정리하는 글입니다.   Decision Tree Decision tree는 마치 순서도 처럼 예&#x2F;아니오 같은 이진으로 분류된 기준을 이어나가면서 분류를 하는 알고리즘 입니다. 회귀도 가능하지만 분류 위주로 정리되었습니다. Decision Tree의 장점은 해석하기 쉽다는데">
<meta property="og:locale">
<meta property="og:image" content="https://jonghwanyoon.github.io/images/hongong_ml_04_fig0.png">
<meta property="og:image" content="https://jonghwanyoon.github.io/images/hongong_ml_04_fig1.png">
<meta property="og:image" content="https://jonghwanyoon.github.io/images/hongong_ml_04_fig1.png">
<meta property="og:image" content="https://jonghwanyoon.github.io/images/hongong_ml_04_fig2.png">
<meta property="og:image" content="https://jonghwanyoon.github.io/images/hongong_ml_04_fig3.png">
<meta property="og:image" content="https://jonghwanyoon.github.io/images/hongong_ml_04_fig4.png">
<meta property="og:image" content="https://jonghwanyoon.github.io/images/hongong_ml_04_fig5.png">
<meta property="article:published_time" content="2023-01-31T14:04:37.000Z">
<meta property="article:modified_time" content="2023-07-03T12:13:27.614Z">
<meta property="article:author" content="Jonghwan Yoon">
<meta property="article:tag" content="혼공학습단">
<meta property="article:tag" content="혼공">
<meta property="article:tag" content="혼공머신">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jonghwanyoon.github.io/images/hongong_ml_04_fig0.png">
  <!-- Canonical links -->
  <link rel="canonical" href="https://jonghwanyoon.github.io/2023/01/31/hongong-ml-04-md/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Jonghwan Yoon" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://jonghwanyoon.github.io/" target="_blank">
          <img class="img-circle img-rotate" src="/images/blue_whale.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">jonghwanyoon</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Seoul, South Korea</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">Home</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">Archives</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">Categories</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">Tags</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">About</span>
          </a>
        </li>
        
      </ul>
      <!-- 
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/jonghwanyoon" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.linkedin.com/in/jonghwan-yoon" target="_blank" title="Linkedin" data-toggle=tooltip data-placement=top><i class="icon icon-linkedin"></i></a></li>
        
    </ul>
 -->
    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      
  <div class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%EC%B1%85/">책</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%EC%B1%85/%ED%98%BC%EC%9E%90-%EA%B3%B5%EB%B6%80%ED%95%98%EB%8A%94-%EC%8B%9C%EB%A6%AC%EC%A6%88/">혼자 공부하는 시리즈</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/">혼공학습단</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Hexo/" style="font-size: 13.5px;">Hexo</a> <a href="/tags/%EB%A6%AC%EB%B7%B0/" style="font-size: 13px;">리뷰</a> <a href="/tags/%EC%B1%85/" style="font-size: 13.5px;">책</a> <a href="/tags/%EC%BD%94%EB%94%A9-%EC%9D%B8%ED%84%B0%EB%B7%B0/" style="font-size: 13px;">코딩 인터뷰</a> <a href="/tags/%ED%98%BC%EA%B3%B5/" style="font-size: 14px;">혼공</a> <a href="/tags/%ED%98%BC%EA%B3%B5C/" style="font-size: 13.5px;">혼공C</a> <a href="/tags/%ED%98%BC%EA%B3%B5S/" style="font-size: 13px;">혼공S</a> <a href="/tags/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0/" style="font-size: 13px;">혼공머신</a> <a href="/tags/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/" style="font-size: 14px;">혼공학습단</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/">혼공학습단</a>
              </p>
              <p class="item-title">
                <a href="/2023/08/20/hongong-c-06/" class="title">혼자 공부하는 C - 포인터</a>
              </p>
              <p class="item-date">
                <time datetime="2023-08-20T06:48:32.000Z" itemprop="datePublished">2023-08-20</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%EC%B1%85/">책</a>
              </p>
              <p class="item-title">
                <a href="/2023/07/13/book-cracking-the-coding-interview/" class="title">(책) 코딩 인터뷰 완전분석</a>
              </p>
              <p class="item-date">
                <time datetime="2023-07-13T11:50:00.000Z" itemprop="datePublished">2023-07-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Hexo/">Hexo</a>
              </p>
              <p class="item-title">
                <a href="/2023/07/03/you-must-add-nojekyll-in-hexo/" class="title">Hexo에서 Github pages의 deploy fail시 .nojekyll을 추가하자.</a>
              </p>
              <p class="item-date">
                <time datetime="2023-07-03T13:29:13.000Z" itemprop="datePublished">2023-07-03</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%EC%B1%85/">책</a><i class="icon icon-angle-right"></i><a class="category-link" href="/categories/%EC%B1%85/%ED%98%BC%EC%9E%90-%EA%B3%B5%EB%B6%80%ED%95%98%EB%8A%94-%EC%8B%9C%EB%A6%AC%EC%A6%88/">혼자 공부하는 시리즈</a>
              </p>
              <p class="item-title">
                <a href="/2023/07/03/hongong-10th-summary/" class="title">(책) 혼자 공부하는 C언어 정리</a>
              </p>
              <p class="item-date">
                <time datetime="2023-07-02T15:00:00.000Z" itemprop="datePublished">2023-07-03</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%EC%B1%85/">책</a><i class="icon icon-angle-right"></i><a class="category-link" href="/categories/%EC%B1%85/%ED%98%BC%EC%9E%90-%EA%B3%B5%EB%B6%80%ED%95%98%EB%8A%94-%EC%8B%9C%EB%A6%AC%EC%A6%88/">혼자 공부하는 시리즈</a>
              </p>
              <p class="item-title">
                <a href="/2023/03/09/hongong-9th-summary/" class="title">(책) 혼자 공부하는 ML/DL, SQL 정리</a>
              </p>
              <p class="item-date">
                <time datetime="2023-03-09T11:54:43.000Z" itemprop="datePublished">2023-03-09</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-hongong-ml-04-md" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      ML | Decision Tree, Cross validation, Ensemble
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2023/01/31/hongong-ml-04-md/" class="article-date">
	  <time datetime="2023-01-31T14:04:37.000Z" itemprop="datePublished">2023-01-31</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/">혼공학습단</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%ED%98%BC%EA%B3%B5/" rel="tag">혼공</a>, <a class="article-tag-link-link" href="/tags/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0/" rel="tag">혼공머신</a>, <a class="article-tag-link-link" href="/tags/%ED%98%BC%EA%B3%B5%ED%95%99%EC%8A%B5%EB%8B%A8/" rel="tag">혼공학습단</a>
  </span>


        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2023/01/31/hongong-ml-04-md/#comments" class="article-comment-link">Comments</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <p><br><br></p>
<p>decision tree, cross validation, ensemble에 대해 정리하는 글입니다.</p>
<p><br><br></p>
<h1 id="decision-tree"><a class="markdownIt-Anchor" href="#decision-tree"></a> Decision Tree</h1>
<p><code>Decision tree</code>는 마치 순서도 처럼 예/아니오 같은 이진으로 분류된 기준을 이어나가면서 분류를 하는 알고리즘 입니다. 회귀도 가능하지만 분류 위주로 정리되었습니다. <code>Decision Tree</code>의 장점은 해석하기 쉽다는데 있습니다.</p>
<br>
<h2 id="와인-분류하기"><a class="markdownIt-Anchor" href="#와인-분류하기"></a> 와인 분류하기</h2>
<p><code>Decision Tree</code>의 장점을 확인하기 위해 <code>Logistic Regression</code>의 분류와 비교해보겠습니다.</p>
<p>아래의 데이터로 실습을 해보겠습니다. 와인의 알콜 농도, 당도, pH와 와인의 클래스입니다.</p>
<p>클래스가 0 이면 <font color="red">레드</font>와인, 1이면 <font color="gray">화이트</font> 와인입니다.</p>
<p>와인 클래스를 예측하는 모델을 <code>Logistic Regression</code>과 <code>Decision Tree</code>로 각각 진행해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">wine = pd.read_csv(<span class="string">&quot;https://bit.ly/wine_csv_data&quot;</span>)</span><br><span class="line">wine</span><br></pre></td></tr></table></figure>
<center>
<table>
<thead>
<tr>
<th>Index</th>
<th>alcohol</th>
<th>sugar</th>
<th>pH</th>
<th>class</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>9.4</td>
<td>1.9</td>
<td>3.51</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>9.8</td>
<td>2.6</td>
<td>3.2</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>9.8</td>
<td>2.3</td>
<td>3.26</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>9.8</td>
<td>1.9</td>
<td>3.16</td>
<td>0</td>
</tr>
<tr>
<td>4</td>
<td>9.4</td>
<td>1.9</td>
<td>3.51</td>
<td>0</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>6492</td>
<td>11.2</td>
<td>1.6</td>
<td>3.27</td>
<td>1</td>
</tr>
<tr>
<td>6493</td>
<td>9.6</td>
<td>8</td>
<td>3.15</td>
<td>1</td>
</tr>
<tr>
<td>6494</td>
<td>9.4</td>
<td>1.2</td>
<td>2.99</td>
<td>1</td>
</tr>
<tr>
<td>6495</td>
<td>12.8</td>
<td>1.1</td>
<td>3.34</td>
<td>1</td>
</tr>
<tr>
<td>6496</td>
<td>11.8</td>
<td>0.8</td>
<td>3.26</td>
<td>1</td>
</tr>
</tbody>
</table>
</center>
<br>
<p><strong>로지스틱 회귀로 와인 분류하기</strong></p>
<p>train과 test를 나누고 Standardization을 해줍니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data = wine[[<span class="string">&quot;alcohol&quot;</span>, <span class="string">&quot;sugar&quot;</span>, <span class="string">&quot;pH&quot;</span>]]</span><br><span class="line">target = wine[<span class="string">&quot;class&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">train_input, test_input, train_target, test_target = train_test_split(</span><br><span class="line">    data, target, test_size = <span class="number">0.2</span>, random_state = <span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">ss.fit(train_input)</span><br><span class="line"></span><br><span class="line">train_scaled = ss.transform(train_input)</span><br><span class="line">test_scaled = ss.transform(test_input)</span><br></pre></td></tr></table></figure>
<br>
<p><strong>LogisticRegression</strong>으로 분류하여 성능을 확인합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(train_scaled, train_target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lr.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(lr.score(test_scaled, test_target))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.7808350971714451</span></span><br><span class="line"><span class="comment"># 0.7776923076923077</span></span><br></pre></td></tr></table></figure>
<br>
<p>train셋은 0.78, test셋은 0.77의 정확도를 보입니다. 성능은 좋아보이지 않습니다. 선형 방정식이 어떻게 학습되었는지 확인해보겠습니다.</p>
<br>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">dict</span>(<span class="built_in">zip</span>(train_input.columns, lr.coef_[<span class="number">0</span>])))</span><br><span class="line"><span class="built_in">print</span>(lr.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;&#x27;alcohol&#x27;: 0.512702742045543, &#x27;sugar&#x27;: 1.6733910972911463, &#x27;pH&#x27;: -0.6876778082262984&#125;</span></span><br><span class="line"><span class="comment"># [1.81777902]</span></span><br></pre></td></tr></table></figure>
<br>
<p>데이터에 대한 weight와 intercept는 다음처럼 나타났습니다. 이를 해석하기는 어렵습니다. 우리는 이 수치를 보고는 알코올과 당도가 높을 수록 화이트 와인일 가능성이 높고, pH가 높을 수록 레드와인일 가능성이 높다고만 생각할 순 있지만 수치가 어떤 의미가 있는지는 알 수 없습니다. 여기서 polynomial로 만든다면 더더욱 알기 어렵습니다.</p>
<p><br><br></p>
<p><strong>Decision Tree</strong>으로 분류를 해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">dt_clf = DecisionTreeClassifier()</span><br><span class="line">dt_clf.fit(train_scaled, train_target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(test_scaled, test_target))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.996921300750433</span></span><br><span class="line"><span class="comment"># 0.86</span></span><br></pre></td></tr></table></figure>
<p>train셋은 0.99, test셋은 0.86의 정확도를 보여줍니다. 점수가 train이 훨씬 높으므로 overfitting이 있는거 같습니다.</p>
<p>decision Tree가 어떻게 분류했는지 확인해보죠.</p>
<p>시각화를 위해 matplotlib와 scikit-learn의 <strong>plot_tree</strong>를 이용합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"></span><br><span class="line">plt.figure(figsize= (<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">plot_tree(dt_clf, rounded=<span class="literal">True</span>, filled = <span class="literal">True</span>, feature_names = [<span class="string">&quot;alcohol&quot;</span>, <span class="string">&quot;suger&quot;</span>, <span class="string">&quot;pH&quot;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/hongong_ml_04_fig0.png" alt="" /></p>
<p>복잡해 보이는 그림이 나왔습니다. decision tree가 결정하는 방식을 보여줍니다. rooted binary tree 구조입니다. 근데 보이지 않으니 <code>max_depth</code>를 설정해서 깊이 1까지만 확인해 보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize= (<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">plot_tree(dt_clf, max_depth = <span class="number">1</span>, rounded=<span class="literal">True</span>, filled = <span class="literal">True</span>, feature_names = [<span class="string">&quot;alcohol&quot;</span>, <span class="string">&quot;suger&quot;</span>, <span class="string">&quot;pH&quot;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/hongong_ml_04_fig1.png" alt="" /></p>
<p>상자 안을 설명하면 다음과 같습니다.</p>
<ul>
<li>테스트 조건 - sugar</li>
<li>불순도 (impurity) - gini</li>
<li>총 샘플 수 - 5197</li>
<li>클래스 별 샘플 수 - [1258, 3939]</li>
</ul>
<p>루트 노드를 예로 들면, 당도가 -0.239 이하가 맞으면 왼쪽 노드로 이동하고, 아니라면 오른쪽 노드를 순회하여 내려갑니다.</p>
<p>각각의 노드에서 한 쪽 클래스를 잘 예측할 수록 색깔을 진하게 표시합니다.</p>
<h2 id="impurity"><a class="markdownIt-Anchor" href="#impurity"></a> Impurity</h2>
<p>scikit-learn의 DecisionTreeClassifier는 impurity를 측정하여 이를 기준으로 나눕니다.</p>
<p>criterion 파라미터를 보면 다음과 같습니다.</p>
<blockquote>
<p>criterion{“gini”, “entropy”, “log_loss”}, default=”gini”<br />
The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain, see <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation">Mathematical formulation</a>.</p>
</blockquote>
<p>수식을 처음부터 살펴 보면 …</p>
<ul>
<li>하나의 숫자로 이루어진 실수를 <strong>스칼라</strong>라고 부르며, $ x $같이 변수로 나타낼 수 있습니다. 실수 집합인 $ R $ 의 원소이고 이를 표시하면 아래와 같습니다.</li>
</ul>
\begin{align}
x \in R
\end{align}

<br>
<ul>
<li>여러 숫자가 특정 순서대로 모여있으면 벡터라고 합니다.<br>train셋을 x로, test셋을 y로 했을때 벡터를 다음처럼 나타낼 수 있습니다.</li>
</ul>
\begin{cases}
\text{train vector}=x_i \in R^n &(i=1, ..., l) \\\
\text{target vector}=y \in R^l
\end{cases}

<ul>
<li>
<p>주어진 train, target 벡터에 대해 decision tree는 같은 target class를 가진 데이터나 비슷한 target class끼리 sub 그룹을 만듭니다.</p>
</li>
<li>
<p>노드 $ m $에 대해 분류에 사용할 $ n_m $ 개의 샘플의 집합을 $ Q_m $으로 표현합니다.</p>
</li>
<li>
<p>feature $ j $에 대한 threshold $ t_m $ 으로 분류할 candidate split은 $ \theta = (j, t_m) $ 으로 나타냅니다.</p>
</li>
<li>
<p>분류될 샘플들은 $ Q_m^{left}(\theta)$ 와 $ Q_m^{right}(\theta) $ 으로 subset이 되어 나누어집니다.</p>
</li>
</ul>
\begin{cases}

Q_m^{left}(\theta) = {(x, y)|x_j \le t_m}\\\
Q_m^{right}(\theta) = Q_m \setminus Q_m^{left}(\theta)

\end{cases}

<blockquote>
<p>$ A \setminus B $ 는 차집합<sup>set difference</sup>으로 $ B $ 의 원소가 포함되지 않는 $ A $ 의 원소를 말합니다.</p>
</blockquote>
<p>잠깐 <strong>정리</strong>를 하면, 노드 안에서는 분류할 데이터 집합을 Q로 나타내고, feature의 분류 기준 (threshold)에 따라 $ Q^{left} $, $ Q^{right} $ 로 나누어 집니다.</p>
<p>이제 여기서, node $ m $의 candidate split에 대한 quality를 함수 $ H() $ 로 계산합니다. 함수의 종류는 해결하고자 하는 모델 종류에 따라 달라집니다. 보통은 gini 와 entropy를 사용합니다.</p>
<p>클래스 K개가 0, 1, 2, … K-1 이고, 노드 $ m $ 에 대한 클래스의 비율을 나타내면 다음과 같습니다.</p>
\begin{align}
p_{mk} = \frac {1} {n_m} \sum_{y\in{Q_m}}I(y=k)
\end{align}

<p><strong>gini</strong></p>
\begin{align}
H(Q_m) = 1 - \sum_{m} (p_{mk})^2
\end{align}

<ul>
<li>쉽게 말하면 클래스의 비율을 제곱해서 모두 더하고 1을 빼면 됩니다.</li>
<li>이 값은 0~0.5 사이로 나타나며, 0에 가까울 수록 분류가 잘되고, 0.5에 가까울 수록 분류가 되지 않습니다.</li>
<li>0은 (completely) pure node라고 합니다.</li>
</ul>
<br>
<p><strong>entropy</strong></p>
\begin{align}

H(Q_m) = -\sum_{m}p_{mk}\log_{2}(p_{mk})

\end{align}

<ul>
<li>entropy도 클래스의 비율을 사용하지만 log2를 이용합니다.</li>
<li>이 값은 0~1 사이로 나타나며, 0에 가까울 수록 분류가 잘되고, 1에 가까울 수록 분류가 되지 않습니다.</li>
<li>0은 (completely) pure node라고 합니다.</li>
</ul>
<p>다시 <strong>정리</strong>하면, loss function $ H $ 를 이용해 클래스의 비율에 따라 잘 분류했는지에 대한 정보인 impurity를 계산합니다.</p>
<p>이제 impurity를 child 노드에 대해 계산합니다.</p>
\begin{align}
G(Q_m, \theta) = \frac {n_m^{left}} {n_m} H(Q_m^{left}(\theta)) + \frac {n_m^{right}} {n_m} H(Q_m^{right}(\theta))
\end{align}

<p>parent 노드와의 impurity 차이를 계산하여 구하고, 이를 <strong>information gain</strong> 이라고 합니다. impurity를 최소화 하는 파라미터를 찾아서 선택합니다.</p>
\begin{align}
\theta^* = {\arg}{\min}_\theta G(Q_m, \theta)
\end{align}

<p>다시 그림으로 돌아와서…</p>
<p><img src="/images/hongong_ml_04_fig1.png" alt="" /></p>
<p>sklearn에서는 기본으로 gini를 사용해 impurity를 계산했습니다.</p>
<p>root node를 기준으로</p>
<ul>
<li>전체 샘플 수는 5197 이고, subset은 1258, 3939개로 나누어집니다.</li>
<li>gini를 계산하면 다음과 같습니다.</li>
</ul>
\begin{align}
1 - ((\frac {1258} {5197})^2 +(\frac {3939} {5197})^2) = 0.397 
\end{align}

<p><strong>Pruning</strong></p>
<p>decision tree가 child node를 계속 학습해서 무한히 내려가면 어떻게 될까요? 끝까지 가면 train셋에는 분명 좋은 성능을 보일 것이지만, overfitting이 될 가능성이 높습니다. 첫 번째 그림이 바로 그 결과입니다.</p>
<p>이는 decision tree의 단점이고, 적절하게 가지치기 (pruning)을 해야합니다.</p>
<p>DecisionTreeClassfier로 처음 모델을 만들 때, max_depth를 이용해 설정할 수 있습니다. 3으로 지정하여 실습해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dt_clf = DecisionTreeClassifier(max_depth=<span class="number">3</span>)</span><br><span class="line">dt_clf.fit(train_scaled, train_target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(test_scaled, test_target))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.8454877814123533</span></span><br><span class="line"><span class="comment"># 0.8415384615384616</span></span><br></pre></td></tr></table></figure>
<p>train셋에 대해서 점수는 떨어졌지만 test셋은 그대로이고 두 데이터셋의 차이가 없어보입니다. 이제 tree를 그려보면 다음과 같습니다.</p>
<p><img src="/images/hongong_ml_04_fig2.png" alt="" /></p>
<p>max_depth가 설정한대로 3까지만 존재합니다.<br />
하지만 아직 설명하기 어려운 부붕니 있습니다. root node를 기준으로 보면, 표준화된 당도의 값이 -0.239보다 낮고, … 라고 말하는 것은 해석에 어려움이 있습니다. decision tree의 장점 중 하나는 표준화 전처리 여부는 decision tree에 영향을 주지 않는 것입니다. impurity는 나누어진 클래스의 비율만을 가지고 계산하기 때문이죠.</p>
<p>표준화 전처리 하지 않은 데이터로 다시 score를 계산하고 plotting을 해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dt_clf = DecisionTreeClassifier(max_depth=<span class="number">3</span>)</span><br><span class="line">dt_clf.fit(train_input, train_target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(train_input, train_target))</span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(test_input, test_target))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.8454877814123533</span></span><br><span class="line"><span class="comment"># 0.8415384615384616</span></span><br></pre></td></tr></table></figure>
<p>스코어가 그대로 인것을 확인할 수 있습니다.</p>
<p><img src="/images/hongong_ml_04_fig3.png" alt="" /></p>
<p>impurity 값들도 그대로입니다. 가장 좋은것은 당도, 알콜농도, pH를 있는 그대로의 기준으로 설명할 수 있습니다.</p>
<p>당도가 1.625보다 크고 4.324보다 작은 와인에서 도수가 11.025 이하라면 레드와인, 아니라면 화이트 와인으로 분류합니다.</p>
<p>그리고 각각의 특성에 대한 중요도를 <code>feature_importances_</code> 메써드로 확인할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dict</span>(<span class="built_in">zip</span>(train_input.columns, dt_clf.feature_importances_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;&#x27;alcohol&#x27;: 0.12345625703073809,</span></span><br><span class="line"><span class="comment">#  &#x27;sugar&#x27;: 0.8686293409940407,</span></span><br><span class="line"><span class="comment">#  &#x27;pH&#x27;: 0.007914401975221242&#125;</span></span><br></pre></td></tr></table></figure>
<p>결과는, 당도가 와인을 잘 구분할 수 있는 기준이 된다는 것을 알려줍니다. 이렇게 feature 별로 중요도를 통해 feature selection에 활용할 수 있습니다.</p>
<br>
<h1 id="교차검증과-그리스-서치"><a class="markdownIt-Anchor" href="#교차검증과-그리스-서치"></a> 교차검증과 그리스 서치</h1>
<p>지금까지 데이터셋을 훈련세트과 테스트세트로 나누고 훈련세트를 통해 학습시킨 모델로 테스트세트를 평가했습니다.</p>
<p>그런데 테스트세트에 대한 성능을 계속 확인하면서 모델을 만든다면, <strong>점점 테스트세트에 맞춘 모델이 됩니다</strong>.</p>
<blockquote>
<p>머신러닝이 아닌 직접 만든 알고리즘의 경우, 이런 사례가 많습니다…😮😮</p>
</blockquote>
<p>그러므로, 만들어진 모델에 대해서 테스트세트의 성능으로 <strong>앞으로의 성능도 이럴 것이다라는 일반화</strong>를 신뢰하려면, 테스트세트는 가능한한 사용하지 않고, 마지막 검증에만 사용하는것이 좋습니다.</p>
<p>하지만, 테스트세트를 사용하지 않는다면 overfitting, underfitting을 확인할 수 없습니다.</p>
<p>이 경우 검증 세트<sup>validation set</sup>가 해결해 줄수 있습니다.</p>
<p><br><br></p>
<center>
<img width=600px src=https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/ML_dataset_training_validation_test_sets.png/800px-ML_dataset_training_validation_test_sets.png?20201206114522>
<br>
<p><font color=gray><a target="_blank" rel="noopener" href="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/ML_dataset_training_validation_test_sets.png/800px-ML_dataset_training_validation_test_sets.png?20201206114522">Train, Validation, Test set</a><br></font></center></p>
<p><br><br></p>
<p>아주 간단하게 훈련세트에서 또다시 일부를 나눈 것입니다. 보통 20~30%를 테스트세트와 검증세트로 나눕니다.</p>
<p>위에서 사용한 데이터를 그대로 사용해보겠습니다.</p>
<br>
<p><strong>검증세트 만들기</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sub_input, val_input, sub_target, val_target = train_test_split(</span><br><span class="line">    train_input, train_target, test_size = <span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sub_input.shape, val_input.shape)</span><br><span class="line"><span class="comment"># (4157, 3) (1040, 3)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><em>train_test_split에 train_input을 넣었습니다</em></p>
</blockquote>
<br>
<p><strong>훈련세트와 검증세트로 score확인</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dt_clf = DecisionTreeClassifier()</span><br><span class="line">dt_clf.fit(sub_input, sub_target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(sub_input, sub_target))</span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(val_input, val_target))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9971133028626413</span></span><br><span class="line"><span class="comment"># 0.8586538461538461</span></span><br></pre></td></tr></table></figure>
<p>점수를 보니 훈련세트에 overfitting되어 있음을 확인할 수 있었습니다.</p>
<br>
<h3 id="교차-검증"><a class="markdownIt-Anchor" href="#교차-검증"></a> 교차 검증</h3>
<p>검증세트가 왜 사용하는지는 이해가 됩니다만, 이러면 훈련세트의 크기가 줄어들어서 학습이 제대로 되지 않을 것 같습니다. 그렇다고 검증세트의 크기를 줄이면 제대로 검증이 되지 않을 것입니다.</p>
<p>그래서 검증세트는 기본적으로 교차검증<sup>Cross validation</sup>을 이용합니다.</p>
<p><br><br></p>
<center>
<img width=800px src=/images/hongong_ml_04_mission.png>
<br>
<p><font color=gray>교차 검증의 예시<br></font></center></p>
<p><br><br></p>
<p>교차검증이란, 검증세트를 나누어 평가하는 과정을 여러번 반복합니다. 보통 K번 반복하는 교차검증을 <strong>K-fold cross validation</strong>이라고 합니다.</p>
<p>위 그림에서 검정색 네모칸을 검증세트에 대해서 K번 평가를 합니다. 그리고 보통 <strong>평가된 점수의 평균</strong>을 대표값으로 사용합니다.</p>
<p>scikit-learn에서는 이를 자동으로 해주는 cross_validate라는 함수를 제공합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_validate</span><br><span class="line"></span><br><span class="line">dt_clf = DecisionTreeClassifier()</span><br><span class="line">scores = cross_validate(dt_clf, train_input, train_target)</span><br><span class="line"><span class="built_in">print</span>(scores)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;&#x27;fit_time&#x27;: array([0.00964594, 0.00899792, 0.00821495, 0.00859427, 0.00850391]),</span></span><br><span class="line"><span class="comment">#  &#x27;score_time&#x27;: array([0.002074  , 0.0016861 , 0.00194216, 0.00178385, 0.00165391]),</span></span><br><span class="line"><span class="comment">#  &#x27;test_score&#x27;: array([0.87115385, 0.85576923, 0.87776708, 0.85466795, 0.8373436 ])&#125;</span></span><br></pre></td></tr></table></figure>
<p>fit_time, score_time, test_score를 가진 dictionary를 반환합니다. cross_validate는 5-fold가 default이므로 값도 5개씩 있습니다. <code>cv</code> 매개변수로 Fold 수를 바꿀 수 있습니다.</p>
<ul>
<li>fit_time: 훈련하는 시간</li>
<li>score_time: 검증하는 시간</li>
<li>test_score: 검증 점수</li>
</ul>
<p>검증 점수의 평균은 다음과 같습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;test_score&quot;</span>].mean())</span><br><span class="line"><span class="comment"># 0.8545302435774044</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>여기서는 train_input과 train_target에 대해 cross_validation을 했으므로, test_score의 점수는 이름은 test지만 검증세트라고 보면 됩니다.</p>
</blockquote>
<p><strong>Splitter</strong></p>
<p>지금까지는 train_test_split으로 전체 데이터를 섞고 훈련세트를 뽑아서 섞을 필요는 없었지만, 교차 검증을 할 때, 훈련 세트를 섞으려면 splitter를 지정해야 합니다.</p>
<ul>
<li>Regression: KFold</li>
<li>Classification: StratifiedKFold</li>
</ul>
<p>지금은 분류이므로 StratifiedKFold를 사용해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line">scores = cross_validate(dt_clf, train_input, train_target, cv = StratifiedKFold())</span><br><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;test_score&quot;</span>].mean())</span><br><span class="line"><span class="comment"># 0.8601106833493745</span></span><br></pre></td></tr></table></figure>
<br>
<p><code>cross_validate</code> 함수의 cv 매개변수로 <code>StratifiedKFold()</code>를 넣어주면 됩니다.</p>
<p>기본은 5 fold이고, fold수를 바꾸려면 StratifiedKFold의 <code>n_splits</code>를 바꾸어주면 됩니다.</p>
<br>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">splitter = StratifiedKFold(n_splits = <span class="number">10</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">42</span>)</span><br><span class="line">scores = cross_validate(dt_clf, train_input, train_target, cv = splitter)</span><br><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;test_score&quot;</span>].mean())</span><br><span class="line"><span class="comment"># 0.8604972580406105</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p>좋습니다. 교차검증을 진행하면서도 한층 간결한 코드를 사용할 수 있게 되었습니다.</p>
<p>그런데 문제가 하나 있습니다. max_depth같은 사용자가 지정하는 옵션은 어떻게 최적값을 찾을까요?</p>
<p><br><br></p>
<h3 id="하이퍼파라미터-튜닝"><a class="markdownIt-Anchor" href="#하이퍼파라미터-튜닝"></a> 하이퍼파라미터 튜닝</h3>
<p>decision tree에서 max_depth 같은 유저가 직접 지정하는 파라미터를 <strong>하이퍼파라미터</strong> 라고 합니다.</p>
<p>중요한 점은 모델마다 여러 종류의 하이퍼파라미터를 가질 수 있다는 점이며, 하나의 하이퍼파라미터를 최적화 했다고 고정값이 될수 없다는 것입니다. 예를 들어 decision tree의 max_depth의 최적값을 찾은 다음에 다시  min_samples_split의 최적값을 찾는 것은 의미가 없습니다. 두 파라미터에 대해서 동시에 최적값을 찾아야 합니다.</p>
<p>이를 <code>GridSearchCV</code> 클래스가 편리하게 해결해줍니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">params = <span class="built_in">dict</span>(min_impurity_decrease = np.arange(<span class="number">0.0001</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>),</span><br><span class="line">              max_depth = <span class="built_in">range</span>(<span class="number">5</span>, <span class="number">20</span>, <span class="number">1</span>),</span><br><span class="line">              min_samples_split = <span class="built_in">range</span>(<span class="number">2</span>, <span class="number">100</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">gs = GridSearchCV(DecisionTreeClassifier(random_state = <span class="number">42</span>),</span><br><span class="line">                  param_grid = params,</span><br><span class="line">                  n_jobs = <span class="number">4</span>)</span><br><span class="line">gs.fit(train_input, train_target)</span><br></pre></td></tr></table></figure>
<ul>
<li>하이퍼파라미터는 dictionary 형태로 GridSearchCV에는 param_grid에 넣어줍니다. (positional input으로 2번째에 key 없이 넣어줘도 됩니다.)</li>
<li>n_jobs는 사용할 병렬 컴퓨팅에 사용할 CPU수를 지정할 수 있습니다. default는 1이고, -1은 가용한 자원을 모두 사용합니다.<br>params에 3가지 하이퍼파라미터를 동시에 교차검증을 하므로, 학습은 $ 10x15x10 = 1350 $ 번 하게 되고, 5-fold가 기본이므로 6750개의 모델을 생성합니다. 따라서 하이퍼파라미터를 튜닝할 때에는 사용할 CPU수를 적절히 지정해줘야합니다.</li>
</ul>
<br>
<p>가장 잘찾은 파라미터 조합은 best_params_ 으로 찾을 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(gs.best_params_)</span><br><span class="line"><span class="comment"># &#123;&#x27;max_depth&#x27;: 14, &#x27;min_impurity_decrease&#x27;: 0.0004, &#x27;min_samples_split&#x27;: 12&#125;</span></span><br></pre></td></tr></table></figure>
<br>
<p>가장 좋은 교차검증 점수는 <code>cv_results_[&quot;mean_test_score&quot;]</code>의 최대값으로 볼 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.<span class="built_in">max</span>(gs.cv_results_[<span class="string">&quot;mean_test_score&quot;</span>]))</span><br><span class="line"><span class="comment"># 0.8683865773302731</span></span><br></pre></td></tr></table></figure>
<br>
<p>가장 좋은 점수의 모델을 <code>best_estimator_</code>으로 저장되어 있고 바로 decision tree 모델처럼 사용할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dt_clf = gs.best_estimator_</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(train_input, train_target))</span><br><span class="line"><span class="comment"># 0.892053107562055</span></span><br><span class="line"><span class="built_in">print</span>(dt_clf.score(test_input, test_target))</span><br><span class="line"><span class="comment"># 0.8615384615384616</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<h3 id="랜덤-서치"><a class="markdownIt-Anchor" href="#랜덤-서치"></a> 랜덤 서치</h3>
<p>하이퍼파라미터를 수치로 입력해야 할 때, 적절한 값의 범위를 알기 어려울 수 있습니다.<br />
예를 들어, min_impurity_decrease 의 기본값은 0.0001 입니다. 대충 0.001 정도를 최대로 해본다고 해도, 0.0001 단위로 할지 0.0002 단위로 할지는 애매합니다.</p>
<p>게다가 너무 많은 값을 그리드서치에 사용하면 수행 시간이 오래 걸릴 수 있습니다.</p>
<p>이럴 때, <strong>랜덤 서치</strong>를 사용하면 됩니다.</p>
<p>파라미터 딕셔너리에서 각 매개변수의 value에 랜덤 샘플링을 해주는 함수를 넣어주면 되고 <strong>GridSearchCV</strong>가 아닌 <strong>RandomizedSearchCV</strong>를 사용하면 됩니다. scipy의 uniform과 randint를 사용하면 됩니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> uniform, randint</span><br><span class="line"></span><br><span class="line">rgen = randint(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(rgen.rvs(<span class="number">10</span>))</span><br><span class="line"><span class="comment"># [6 3 6 2 5 9 7 8 8 7]</span></span><br><span class="line"></span><br><span class="line">ugen = uniform(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(ugen.rvs(<span class="number">10</span>))</span><br><span class="line"><span class="comment"># [0.74803462 0.10100137 0.88553271 0.1432955  0.1551121  0.16308304 0.44705319 0.25044722 0.66360482 0.10661267]</span></span><br></pre></td></tr></table></figure>
<p>너무 작은 값이지만 랜덤하게 잘 뽑아 줍니다.</p>
<br>
<br>
<p><strong>랜덤하게 뽑은 10000개 정수의 분포</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int_array = randint(<span class="number">0</span>, <span class="number">10</span>).rvs(<span class="number">10000</span>)</span><br><span class="line">pd.DataFrame(int_array).plot.hist()</span><br></pre></td></tr></table></figure>
<p><img src="/images/hongong_ml_04_fig4.png" alt="" /></p>
<br>
<p><strong>랜덤하게 뽑은 10000개 실수의 분포</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">float_array = uniform(<span class="number">0</span>, <span class="number">1</span>).rvs(<span class="number">10000</span>)</span><br><span class="line">pd.DataFrame(float_array).plot.hist()</span><br></pre></td></tr></table></figure>
<p><img src="/images/hongong_ml_04_fig5.png" alt="" /></p>
<br>
<p><strong>RandomizedSearchCV 사용해보기</strong></p>
<p>아래 params의 각 value는 uniform과 randint로 설정했습니다. 그리고 랜덤으로 뽑아서 수행하기 때문에, RandomizedSearchCV의 n_iter를 설정하여 몇 번 수행할지 정해줘야 합니다. 기본은 10번이고, 여기서는 100으로 해보겠습니다.</p>
<blockquote>
<p>GridSearchCV와 달리 param_grid는 positional input으로 넣어줘야 합니다.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"></span><br><span class="line">params = <span class="built_in">dict</span>(min_impurity_decrease = uniform(<span class="number">0.0001</span>, <span class="number">0.001</span>),</span><br><span class="line">              max_depth = randint(<span class="number">5</span>, <span class="number">20</span>),</span><br><span class="line">              min_samples_split = randint(<span class="number">2</span>, <span class="number">25</span>),</span><br><span class="line">              min_samples_leaf = randint(<span class="number">1</span>, <span class="number">25</span>))</span><br><span class="line"></span><br><span class="line">gs = RandomizedSearchCV(DecisionTreeClassifier(random_state = <span class="number">42</span>),</span><br><span class="line">                        params,</span><br><span class="line">                        n_iter = <span class="number">100</span>,</span><br><span class="line">                        n_jobs = <span class="number">4</span>)</span><br><span class="line">gs.fit(train_input, train_target)</span><br></pre></td></tr></table></figure>
<br>
<p>최적 파라미터 값의 조합을 출력해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(gs.best_params_)</span><br><span class="line"><span class="comment"># &#123;&#x27;max_depth&#x27;: 39, &#x27;min_impurity_decrease&#x27;: 0.00034102546602601173, &#x27;min_samples_leaf&#x27;: 7, &#x27;min_samples_split&#x27;: 13&#125;</span></span><br></pre></td></tr></table></figure>
<br>
<p>가장 높은 교차검증 점수도 확인해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.<span class="built_in">max</span>(gs.cv_results_[<span class="string">&quot;mean_test_score&quot;</span>]))</span><br><span class="line"><span class="comment"># 0.8695428296438884</span></span><br></pre></td></tr></table></figure>
<br>
<p>최적 모델을 가져오고 테스트셋의 성능을 확인해봅니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dt = gs.best_estimator_</span><br><span class="line"><span class="built_in">print</span>(dt.score(test_input, test_target))</span><br><span class="line"><span class="comment"># 0.86</span></span><br></pre></td></tr></table></figure>
<br>
<p>이제, 여러 파라미터로 모델을 만들때, 충분히 다양하게 시도해 볼수 있다고 말할 수 있습니다.</p>
<p>게다가 한정된 자원에서 랜덤하게 파라미터값을 조사해 효율적으로 최적 파라미터도 찾을 수 있게 되었습니다.</p>
<p><br><br></p>
<h1 id="트리의-앙상블"><a class="markdownIt-Anchor" href="#트리의-앙상블"></a> 트리의 앙상블</h1>
<p>이번에는 decision tree를 확장해서 머신러닝에서 아주아주 많이 사용되는 앙상블 모델을 보겠습니다.</p>
<br>
<h2 id="랜덤-포레스트"><a class="markdownIt-Anchor" href="#랜덤-포레스트"></a> 랜덤 포레스트</h2>
<p>랜덤 포레스트는 decision tree를 랜덤하게 여러개를 만들의 <strong>숲</strong>을 만듭니다. 그리고 각 Tree의 예측을 사용해 최종 예측을 만드는 알고리즘입니다.</p>
<p><strong>부트스트랩<sup>Bootstrap</sup></strong> 샘플이란 데이터 집합에서 복원추출한 샘플입니다. 이를 여러번 복원 추출 하는 과정을 리샘플링<sup>Resampling</sup>이라고 합니다. 예를 들어 1000개의 샘플에서 1개를 뽑고 다시 돌려놓는 것이 복원 추출인데, 이를 100번 반복해 얻은 샘플입니다. 따라서 <strong>중복된 샘플도 허용</strong>됩니다. 부트스트랩 샘플은 단지 이렇게 뽑은 결과가 원래 표본과 비슷하다고 생각하고 진행합니다.</p>
<p>랜덤 포레스트는 부트스트랩 샘플을 이용해 각각의 Tree를 학습시킵니다.</p>
<p>scikit-learn에서는 분류와 회귀에 대해 아래 모델이 있습니다.</p>
<br>
<ul>
<li>
<p>RandomForestClassifer</p>
<ul>
<li>일부를 무작위로 골라서 이 중에서 최적을 찾아서 노드를 분할합니다. 전체 feature 개수의 제곱근 만큼의 특성을 선택합니다. 즉, 4개의 feature가 있을 떄 2개만 사용합니다.</li>
<li>각 Tree의 클래스별 확률의 평균을 대표값으로 가장 높은 확률을 가진 클래스를 예측값으로 사용합니다.</li>
</ul>
</li>
<li>
<p>RandomForestRegressor</p>
<ul>
<li>전체 feature를 모두 사용합니다.</li>
<li>각 트리의 예측값의 평균을 대표값으로 사용합니다.</li>
</ul>
</li>
</ul>
<br>
<p>랜덤포레스트는 decision tree의 하이퍼파라미터를 모두 제공합니다. 그리고 <strong>훈련데이터의 특성 중요도도 제공</strong>합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rf.fit(train_input, train_target)</span><br><span class="line"><span class="built_in">dict</span>(<span class="built_in">zip</span>(train_input.columns, rf.feature_importances_))</span><br><span class="line"><span class="comment"># &#123;&#x27;alcohol&#x27;: 0.23167441093509658,</span></span><br><span class="line"><span class="comment">#  &#x27;sugar&#x27;: 0.5003984054070982,</span></span><br><span class="line"><span class="comment">#  &#x27;pH&#x27;: 0.26792718365780516&#125;</span></span><br></pre></td></tr></table></figure>
<br>
<p>decision tree 실습에서는 당도가 0.86 의 중요도였는데 여기서는 0.5로 낮아졌습니다. 이는 랜덤포레스트가 특성의 일부를 랜덤하게 선택하여 훈려하기 때문입니다. 하나의 특성에 집중하지 않도록 하고 다른 특성도 학습할 기회를 얻습니다. 결론적으로는 overfitting을 줄이고 일반화 성능을 높이게 됩니다.</p>
<p>추가로, 부트스트랩은 복원추출을 하므로 부트스트랩에 포함되지 않는 샘플이 존재합니다. 이를 OOB<sup>Out of bag</sup> 샘플이라 합니다. 부트스트랩에 포함되지 않았으니 학습에 포함되지 않았겠군요.<br />
랜덤 포레스트는 OOB 샘플을 검증세트처럼 사용해 평가할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestClassifier(oob_score = <span class="literal">True</span>, n_jobs = <span class="number">4</span>, random_state = <span class="number">42</span>)</span><br><span class="line">rf.fit(train_input, train_target)</span><br><span class="line"><span class="built_in">print</span>(rf.oob_score_)</span><br><span class="line"><span class="comment"># 0.8934000384837406</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<h2 id="엑스트라-트리supextra-treesup"><a class="markdownIt-Anchor" href="#엑스트라-트리supextra-treesup"></a> 엑스트라 트리<sup>Extra Tree</sup></h2>
<p>랜덤 포레스트와 비슷하지만, 부트스트랩 샘플을 사용하지 않고 트리를 만들 때 모든 샘플을 사용합니다. 또한 노드를 분할할 때, 가장 좋은 분할이 아닌 무작위로 분할합니다.</p>
<p>무작위이므로 성능은 낮아질 수 있으나, overfitting을 막을 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">et = ExtraTreesClassifier(n_jobs=<span class="number">4</span>, random_state=<span class="number">42</span>)</span><br><span class="line">scores = cross_validate(et, train_input, train_target,</span><br><span class="line">                        return_train_score=<span class="literal">True</span>, n_jobs=<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;train_score&quot;</span>].mean(), scores[<span class="string">&quot;test_score&quot;</span>].mean())</span><br><span class="line"><span class="comment"># 0.9974503966084433 0.8887848893166506</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p>feature importance도 동일하게 얻을 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">et.fit(train_input, train_target)</span><br><span class="line"><span class="built_in">print</span>(et.feature_importances_)</span><br><span class="line"><span class="comment"># [0.20183568 0.52242907 0.27573525]</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<h2 id="그레이디언트-부스팅supgradient-boostingsup"><a class="markdownIt-Anchor" href="#그레이디언트-부스팅supgradient-boostingsup"></a> 그레이디언트 부스팅<sup>Gradient boosting</sup></h2>
<p>depth가 낮은 tree로 binary tree의 오차를 보완하는 방법입니다.</p>
<p>GradientBoostingClassifier는 default로 depth가 3인 tree를 100개 사용합니다. depth가 3이므로 overfitting에 강하고 높은 일반화 성능을 보입니다.</p>
<p>이전에 정리한 <a href="https://jonghwanyoon.github.io/2023/01/24/hongong_ml_03/"><strong>Gradient descent</strong></a>를 사용하여 tree를 앙상블에 추가합니다. 손실 함수는 아래를 사용합니다.</p>
<ul>
<li>classification: logistic loss function</li>
<li>Regression: Mean squared error</li>
</ul>
<p><br><br></p>
<p><strong>실습</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line">gb = GradientBoostingClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">scores = cross_validate(gb, train_input, train_target,</span><br><span class="line">                        return_train_score=<span class="literal">True</span>, n_jobs=<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;train_score&quot;</span>].mean(), scores[<span class="string">&quot;test_score&quot;</span>].mean())</span><br><span class="line"><span class="comment"># 0.8881086892152563 0.8720430147331015</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<p>훈련세트과 테스트세트의 점수를 보니 overfitting이 거의 일어나지 않았습니다.</p>
<p>그리고 <strong>subsample</strong>라는 파라미터가 있는데 훈련세트의 비율을 나타냅니다. 기본값은 1.0인데, 이를 줄이면 그만큼의 비율만 사용하게 되고, 그러면 <strong>stochastic gradient descent</strong>나 <strong>mini batch gradient descent</strong>와 비슷합니다.</p>
<p>성능은 랜덤포레스트보다 높을 수 있지만, 트리를 순서대로 추가하기 때문에 훈련이 오래 걸립니다. 하나씩 훈련하기에 GradientBoostingClassifier에는 병렬 컴퓨팅 파라미터가 없습니다.</p>
<p>그래서 속도와 성능을 더욱 개선한 히스토그램 기반 그레디언트 부스팅이 있습니다.</p>
<p><br><br></p>
<h2 id="히스토그램-기반-그레디언트-부스팅suphistogram-based-gradient-boostingsup"><a class="markdownIt-Anchor" href="#히스토그램-기반-그레디언트-부스팅suphistogram-based-gradient-boostingsup"></a> 히스토그램 기반 그레디언트 부스팅<sup>Histogram-based gradient boosting</sup></h2>
<ul>
<li>정형 데이터 (tabular 데이터 같은)에 대한 ML 알고리즘 중 인기가 가장 높습니다.</li>
<li>이 모델은 input feature를 256개의 구간으로 나누어서, Node를 분할 시 최적 분할을 <strong>빠르게</strong> 찾을 수 있습니다.</li>
<li>HistGradientBoostingClassifier는 일반적으론 default에서 성능이 좋습니다.</li>
<li>tree의 개수를 지정하는데에는 n_estimators 대신 max_iter를 사용합니다. 성능을 높이려면 max_iter를 사용합시다.</li>
</ul>
<br>
<p><strong>실습</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sklearn에서는 아직 테스트 중입니다. enable_hist_gradient_boosting가 필요합니다.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.experimental <span class="keyword">import</span> enable_hist_gradient_boosting</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> HistGradientBoostingClassifier</span><br><span class="line"></span><br><span class="line">hgb = HistGradientBoostingClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">scores = cross_validate(hgb, train_input, train_target,</span><br><span class="line">                        return_train_score=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;train_score&quot;</span>].mean(), scores[<span class="string">&quot;test_score&quot;</span>].mean())</span><br><span class="line"><span class="comment"># 0.9321723946453317 0.8801241948619236</span></span><br></pre></td></tr></table></figure>
<br>
<p>feature importance를 확인해보겠습니다. 여기서는 permutation_importance라는 함수를 사용해보겠습니다. 특성을 랜덤하게 섞고 모델의 성능이 변화하는지 관찰후 어느 특성이 중요한지 계산합니다. 이 모델이 아닌 다른 모델에도 사용 가능합니다.</p>
<br>
<p><strong>permutation_importance</strong></p>
<p>permutation_importance의 return값은 특성 중요도, 중요도의 평균, 표준 편차를 담고 있습니다. 중요도의 비율은 랜덤포레스트와 비슷합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.inspection <span class="keyword">import</span> permutation_importance</span><br><span class="line"></span><br><span class="line">hgb.fit(train_input, train_target)</span><br><span class="line">result = permutation_importance(hgb, train_input, train_target,</span><br><span class="line">                                n_repeats = <span class="number">10</span>, random_state=<span class="number">42</span>, n_jobs = <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(result.importances_mean)</span><br><span class="line"><span class="comment"># [0.08876275 0.23438522 0.08027708]</span></span><br></pre></td></tr></table></figure>
<p><strong>테스트 셋 확인</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hgb.score(test_input, test_target)</span><br><span class="line"><span class="comment"># 0.8723076923076923</span></span><br></pre></td></tr></table></figure>
<br>
<p>마지막으로 scikit-learn이 아닌 다른 <strong>히스토그램 기반 그레디언트 부스팅</strong>을 구현한 <strong>라이브러리</strong>는 xgboost와 lightgb이 있습니다. scikit-learn의 cross validation 함수에서도 사용할 수 있습니다.</p>
<p>xgboost는 다양한 부스팅 알고리즘을 제공해줍니다.</p>
<p>lightGBM은 마이크로소프트에서 만들었는데, 빠르고 최신기술을 많이 적용해 인기가 좋습니다.</p>
<p><br><br></p>
<p>이번에는 decision tree, cross validation, ensemble 알고리즘에 대해 알아보았습니다. 시간이 될때마다 원리와 수식을 추가로 공부하는게 도움이 될 것 같습니다.</p>
<p>다음에는 clustering 및 PCA 같은 비지도 학습에 대해 정리하겠습니다.</p>
<p>읽어주셔서 감사합니다 👋</p>
<p><br><br></p>
<h3 id="ps"><a class="markdownIt-Anchor" href="#ps"></a> <strong>p.s.</strong></h3>
<br>
<center>
<img width=600px src=/images/hongong_gift2.jpeg >
<p>2주차 커피와 3주차 크로플 선물 감사합니다!</p>
</center>
<p><br><br><br><br></p>
<!-- flag of hidden posts -->
      
    </div>
    <div class="article-footer">
      <!-- <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://jonghwanyoon.github.io/2023/01/31/hongong-ml-04-md/" title="ML | Decision Tree, Cross validation, Ensemble" target="_blank" rel="external">https://jonghwanyoon.github.io/2023/01/31/hongong-ml-04-md/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote> -->
<!-- 

<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://jonghwanyoon.github.io/" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/blue_whale.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://jonghwanyoon.github.io/" target="_blank"><span class="text-dark">jonghwanyoon</span><small class="ml-1x"></small></a></h3>
        <div></div>
      </div>
    </figure>
  </div>
</div>
 -->

    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      </div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/jonghwanyoon" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.linkedin.com/in/jonghwan-yoon" target="_blank" title="Linkedin" data-toggle=tooltip data-placement=top><i class="icon icon-linkedin"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        &copy; 2023 Jonghwan Yoon
        
        <div class="publishby">
        	<!-- Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>. -->
            Theme by <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
    <script defer>
    var disqus_config = function () {
        
            this.page.url = 'https://jonghwanyoon.github.io/2023/01/31/hongong-ml-04-md/';
        
        this.page.identifier = 'hongong-ml-04-md';
    };
    (function() { 
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'wg-yoon' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>






    <script defer type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-140384355-2', 'auto');
ga('send', 'pageview');

</script>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>