<!DOCTYPE html>
<html lang=ko>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="naver-site-verification" content="29892ce904ec6054137e0bb84d2f2197d1eb56f4" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>ML | K-means clustering, PCA | jonghwanyoon</title><meta name="robots" content="noindex">
  <meta name="description" content="3가지 과일 이미지를 Clustering으로 모아봅니다.   비지도 학습에 속하는 K-means clustering과 PCA에 대해 정리하는 글입니다.   K-means clustering 데이터를 K개의 군집으로 모으기 위한 알고리즘입니다. 과정   Wikipedia    무작위로 k개의 centroid (클러스터 중심)를 정합니다. 각 샘플에서 가">
<meta property="og:type" content="article">
<meta property="og:title" content="ML | K-means clustering, PCA">
<meta property="og:url" content="https://jonghwanyoon.github.io/2023/02/06/hongong-ml-05/index.html">
<meta property="og:site_name" content="Jonghwan Yoon">
<meta property="og:description" content="3가지 과일 이미지를 Clustering으로 모아봅니다.   비지도 학습에 속하는 K-means clustering과 PCA에 대해 정리하는 글입니다.   K-means clustering 데이터를 K개의 군집으로 모으기 위한 알고리즘입니다. 과정   Wikipedia    무작위로 k개의 centroid (클러스터 중심)를 정합니다. 각 샘플에서 가">
<meta property="og:locale">
<meta property="article:published_time" content="2023-02-06T11:13:35.000Z">
<meta property="article:modified_time" content="2024-02-24T04:43:06.786Z">
<meta property="article:author" content="Jonghwan Yoon">
<meta property="article:tag" content="bioinformatics, godot">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="https://jonghwanyoon.github.io/2023/02/06/hongong-ml-05/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Jonghwan Yoon" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://jonghwanyoon.github.io/" target="_blank">
          <img class="img-circle img-rotate" src="/images/DALLE_whale.webp" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">jonghwanyoon</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Seoul, South Korea</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">Home</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">Archives</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">Categories</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">Tags</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">About</span>
          </a>
        </li>
        
      </ul>
      <!-- 
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/jonghwanyoon" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.linkedin.com/in/jonghwan-yoon" target="_blank" title="Linkedin" data-toggle=tooltip data-placement=top><i class="icon icon-linkedin"></i></a></li>
        
    </ul>
 -->
    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      
  <div class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bioinformatics/">Bioinformatics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%EC%B1%85/">책</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Hexo/" style="font-size: 14px;">Hexo</a> <a href="/tags/samtools-htslib/" style="font-size: 13px;">samtools,htslib</a> <a href="/tags/%EB%A6%AC%EB%B7%B0/" style="font-size: 14px;">리뷰</a> <a href="/tags/%EC%B1%85/" style="font-size: 14px;">책</a> <a href="/tags/%EC%BD%94%EB%94%A9-%EC%9D%B8%ED%84%B0%EB%B7%B0/" style="font-size: 13px;">코딩 인터뷰</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Bioinformatics/">Bioinformatics</a>
              </p>
              <p class="item-title">
                <a href="/2024/02/24/compile-samtools-on-windows/" class="title">htslib과 samtools를 windows에서 compile 하기</a>
              </p>
              <p class="item-date">
                <time datetime="2024-02-24T02:18:46.000Z" itemprop="datePublished">2024-02-24</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%EC%B1%85/">책</a>
              </p>
              <p class="item-title">
                <a href="/2024/01/04/book-plotly-interactive-visualization/" class="title">(책) Plotly로 시작하는 인터랙티브 데이터 시각화 in R &amp; Python</a>
              </p>
              <p class="item-date">
                <time datetime="2024-01-04T12:59:44.000Z" itemprop="datePublished">2024-01-04</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%EC%B1%85/">책</a>
              </p>
              <p class="item-title">
                <a href="/2023/07/13/book-cracking-the-coding-interview/" class="title">(책) 코딩 인터뷰 완전분석</a>
              </p>
              <p class="item-date">
                <time datetime="2023-07-13T11:50:00.000Z" itemprop="datePublished">2023-07-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Hexo/">Hexo</a>
              </p>
              <p class="item-title">
                <a href="/2023/07/03/you-must-add-nojekyll-in-hexo/" class="title">Hexo에서 Github pages의 deploy fail시 .nojekyll을 추가하자.</a>
              </p>
              <p class="item-date">
                <time datetime="2023-07-03T13:29:13.000Z" itemprop="datePublished">2023-07-03</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2023/07/03/hongong-10th-summary/" class="title">(책) 혼자 공부하는 C언어 정리</a>
              </p>
              <p class="item-date">
                <time datetime="2023-07-02T15:00:00.000Z" itemprop="datePublished">2023-07-03</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-hongong-ml-05" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      ML | K-means clustering, PCA
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2023/02/06/hongong-ml-05/" class="article-date">
	  <time datetime="2023-02-06T11:13:35.000Z" itemprop="datePublished">2023-02-06</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    
  </span>

        

        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2023/02/06/hongong-ml-05/#comments" class="article-comment-link">Comments</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <br>
<center>
<iframe src="/html/hongong_ml_05_fig11.html" width="100%" height="500px" frameborder="0" loading="lazy" allowfullscreen></iframe>
<p>3가지 과일 이미지를 Clustering으로 모아봅니다.</p>
</center>
<p><br><br></p>
<p>비지도 학습에 속하는 K-means clustering과 PCA에 대해 정리하는 글입니다.</p>
<br>
<h1 id="k-means-clustering"><a class="markdownIt-Anchor" href="#k-means-clustering"></a> K-means clustering</h1>
<p>데이터를 K개의 군집으로 모으기 위한 알고리즘입니다.</p>
<p><strong>과정</strong></p>
<center>
<img width=300px src=https://upload.wikimedia.org/wikipedia/commons/7/7b/Kmeans_animation_withoutWatermark.gif>
<p><sup><a target="_blank" rel="noopener" href="https://ko.wikipedia.org/wiki/K-%ED%8F%89%EA%B7%A0_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98">Wikipedia</a></sup></p>
</center>
<br>
<ol>
<li>무작위로 k개의 centroid (클러스터 중심)를 정합니다.</li>
<li>각 샘플에서 가장 가까운 cluster center를 찾아서 해당 cluster의 샘플로 지정합니다.</li>
<li>cluster에 속한 샘플의 평균값으로 centriod를 변경합니다.</li>
<li>centroid에 변화가 없을 때 까지 (수렴할때 까지) 2번부터 반복합니다.</li>
</ol>
<br>
<h2 id="실습"><a class="markdownIt-Anchor" href="#실습"></a> 실습</h2>
<p><strong>데이터 준비</strong></p>
<p>데이터를 다운받고 읽습니다. npy 포맷이므로 numpy로 읽습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 다운로드</span></span><br><span class="line">! wget https://bit.ly/fruits_300_data -O fruits_300.npy</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">fruits = np.load(<span class="string">&quot;fruits_300.npy&quot;</span>)</span><br></pre></td></tr></table></figure>
<br>
<p>데이터 구조를 확인합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fruits.shape</span><br><span class="line"><span class="comment"># (300, 100, 100)</span></span><br></pre></td></tr></table></figure>
<p>데이터셋은 3차원이고, 300개의 과일이 100x100의 픽셀로 된 이미지입니다. 과일의 모양새는 아래와 같습니다.</p>
<center>
<iframe src="/html/hongong_ml_05_fig1.html" width="100%" height="320px" frameborder="0" loading="lazy" allowfullscreen></iframe>
<p>과일 종류: 사과, 파인애플, 바나나</p>
</center>
<br>
<p>과일들은 3가지로 되어있습니다. 데이터 순서대로 100개씩 총 300개입니다. 여기서 유사한 과일끼리 묶어보려 합니다.</p>
<p>tabular data와 달리 차원이 존재합니다. 2차원 이미지 형태의 데이터간에 유사함을 파악하여 비교하려면 어떻게 해야할까요?</p>
<p>픽셀 단위로 쭉 나열한 하나의 배열로 만들면 됩니다. numpy의 reshape 메소드를 이용해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fruits_2d = fruits.reshape(-<span class="number">1</span>, <span class="number">100</span>*<span class="number">100</span>)</span><br><span class="line">fruits_2d.shape</span><br><span class="line"><span class="comment"># (300, 10000)</span></span><br></pre></td></tr></table></figure>
<p>reshape는 사용자가 원하는 형태의 array로 만들 수 있습니다.parameter에는 <code>1차원 개수, 2차원 개수, ...</code> 가 입력됩니다. 데이터셋은 300개의 샘플, 너비 100픽셀, 높이 100픽셀인 3차원 이었고, 이를 2차원으로 만들기 위해 2차원에는 100<em>100 (너비</em>높이)를 입력하였고 1차원에는 <code>-1</code>을 넣었습니다. <code>-1</code>은 알아서 맞춰줍니다.</p>
<br>
<p>KMeans로 모델을 만들어보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">km = KMeans(n_clusters = <span class="number">3</span>, random_state = <span class="number">42</span>)</span><br><span class="line">km.fit(fruits_2d)</span><br></pre></td></tr></table></figure>
<br>
<p>각 샘플별로 어떻게 찾았는지 확인해보겠습니다. <code>labels_</code>에는 각 샘플의 레이블이 나타납니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(km.labels_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</span></span><br><span class="line"><span class="comment">#  2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2</span></span><br><span class="line"><span class="comment">#  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0</span></span><br><span class="line"><span class="comment">#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span></span><br><span class="line"><span class="comment">#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span></span><br><span class="line"><span class="comment">#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span></span><br><span class="line"><span class="comment">#  1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span></span><br><span class="line"><span class="comment">#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span></span><br><span class="line"><span class="comment">#  1 1 1 1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(np.unique(km.labels_, return_counts= <span class="literal">True</span>))</span><br><span class="line"><span class="comment"># (array([0, 1, 2], dtype=int32), array([111,  98,  91]))</span></span><br></pre></td></tr></table></figure>
<p>개수별로 카운트해봤을 때에도, 100개씩 묶어주긴 한거 같습니다. 어떻게 나왔는지 그림으로 살펴보겠습니다.</p>
<br>
<p><strong>라벨 0</strong></p>
<img src=/images/hongong_ml_05_fig1.png>
<p><br><br></p>
<p><strong>라벨 1</strong></p>
<img src=/images/hongong_ml_05_fig2.png>
<p><br><br></p>
<p><strong>라벨 2</strong></p>
<img src=/images/hongong_ml_05_fig3.png>
<p><br><br></p>
<p>과일별로 대체로 잘 모여있는것 같군요.</p>
<p>각 라벨의 centroid는 <code>km.cluster_centers_</code>에 2차원으로 존재합니다. 이를 3차원으로 바꾸고 출력하면 다음과 같습니다.</p>
<center>
<iframe src="/html/hongong_ml_05_fig4.html" width="100%" height="320px" frameborder="0" loading="lazy" allowfullscreen></iframe>
<p>각 라벨의 centroid 출력</p>
</center>
<br>
<p>여러가지 회전한 형태의 각각의 과일을 학습해서 평균점을 찾다보니 흐릿하게 뭉개진것 같습니다. 그래도 3가지를 확실히 구분할것 같긴 하네요.</p>
<p>여기서는 K를 3으로 지정해주었습니다. 하지만 실제 데이터에서는 몇 가지의 과일이 있는지 모릅니다. 그러면 <strong>적당한 cluster 개수 K는 어떻게 알아낼까요?</strong></p>
<p>최적의 K 값은 알아내기 어렵습니다. <sub><em>(예전에 지도교수님은 이걸로 책 몇권을 낼 수 있다고도 하신 기억이 나네요)</em></sub></p>
<br>
<p><strong>Elbow</strong></p>
<p>k means clustering은 <strong>centroid와 cluster에 속한 샘플 사이의 거리</strong>를 계산할 수 있습니다. 이 거리의 제곱을 하여 합진 값을 <strong>inertia</strong>라고 하고, 데이터들이 가깝게 모인 정도를 나타냅니다.</p>
<p>scikit-learn의 KMeans 모델로 clustering을 진행하면 intertia_도 같이 저정됩니다.</p>
<br>
<p>실습한 과일 데이터로 K의 개수별로 어떤일이 일어날지 상상해 봅시다…</p>
<ul>
<li><strong>k=1</strong>: 3가지 과일을 1개의 cluster를 찾게 한다면, 데이터의 중심이 될것입니다. 따라서 <strong>intertia는 최대치로 높게 나타날 것</strong>입니다.</li>
<li><strong>k=2</strong>: 3가지 과일 중 2가지를 clustering을 시킨다면, k=1보다는 아니지만 데이터 중 일부는 여전히 centriod와 거리가 멀으므로 <strong>inertia가 높게 나타날 것</strong>입니다.</li>
<li><strong>k=3</strong>: 3가지 과일 중 3가지를 clustering하여 이상적인 cluster를 형성한다면 <strong>inertia가 급격히 줄어들었을 것</strong>입니다.</li>
<li><strong>k=4~</strong> : 3가지 과일에서 4개 이상의 cluster를 만든다면, 작은 cluster가 생길 것이고, k가 높아질수록 <strong>점차 inertia가 낮아 질 것</strong>입니다.</li>
</ul>
<br>
<p>따라서, <font color="blue"><strong>급격히 내려간 지점 (k=2 -&gt; 3)과 완만히 내려간 지점 (k=3 -&gt; 4)를 찾아내야합니다. Elbow 방법으로 이를 확인할 수 있습니다.</strong> </font></p>
<p>확인을 위해 K가 2~7개 일때, inertia의 변화를 보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># K = 2 ~ 6 으로 clustering 후 intertia를 리스트에 저장</span></span><br><span class="line">inertia = []</span><br><span class="line">k_list = [*<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">7</span>)]</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_list:</span><br><span class="line">    km = KMeans(n_clusters=k, random_state = <span class="number">42</span>)</span><br><span class="line">    km.fit(fruits_2d)</span><br><span class="line">    inertia.append(km.inertia_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 그래프 그리기</span></span><br><span class="line">fig = go.Figure()</span><br><span class="line">fig.add_trace(go.Scatter(x = k_list, y = inertia))</span><br><span class="line">fig.update_layout(template = <span class="string">&quot;plotly_white&quot;</span>,</span><br><span class="line">                  xaxis_title = <span class="string">&quot;k&quot;</span>,</span><br><span class="line">                  yaxis_title = <span class="string">&quot;inertia&quot;</span>)</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure>
<iframe src="/html/hongong_ml_05_fig5.html" width="100%" height="500px" frameborder="0" loading="lazy" allowfullscreen></iframe>
<p>기울기를 볼때 k=3에서 변화가 보이시나요?</p>
<p>저 부분을 팔꿈치 같다고 하여 <strong>elbow</strong> 지점이라 합니다. 이 데이터는 큰 변화는 아니지만 elbow 지점보다 많은 clustering을 시킨다면 inertia가 계속 줄어들게 됩니다.</p>
<p><br><br></p>
<h1 id="principle-component-analysis-suppcasup"><a class="markdownIt-Anchor" href="#principle-component-analysis-suppcasup"></a> Principle Component Analysis <sup>PCA</sup></h1>
<p>PCA는 여러개의 데이터 feature에서 패턴을 알려주고, 어떤 feature가 중요한지와, 해당 feature가 얼마나 데이터를 잘 설명해 주는지 알수 있습니다.</p>
<p>PCA에 대한 이론적인 부분은 제가 좋아하는 유튜브 채널의 영상을 올려드립니다. (한글 자막도 있어요)</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FgakZw6K1QQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p><br><br></p>
<p><strong>실습</strong></p>
<p>scikit-learn에서는 PCA 클래스를 제공합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">50</span> )</span><br><span class="line">pca.fit(fruits_2d)</span><br></pre></td></tr></table></figure>
<br>
<p>n_components를 50으로 지정하였습니다. 주성분<sup>Principal component</sup> 벡터를 50개 찾는 것입니다. 찾은 값은 components_에 들어있습니다. 구조를 확인해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pca.components_.shape</span><br><span class="line"><span class="comment"># (50, 10000)</span></span><br></pre></td></tr></table></figure>
<br>
<p>50개의 주성분과 10000개의 feature (100x100)가 들어있습니다.</p>
<p>그림으로 표현하면 아래와 같습니다.</p>
<img src=/images/hongong_ml_05_fig6.png>
<p>왼쪽 위에서 오른쪽으로 순서대로, 원본 데이터에서 데이터를 잘 표현하는 벡터입니다.</p>
<p>이것이 의미하는 것은 100x100개의 차원이었던 feature에서 가장 잘 설명할 수 있는 50개를 선별한 것입니다. 이제 원본데이터를 주성분 벡터에 투영하면 특성의 개수를 100x100개에서 50개로 줄일 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fruits_pca = pca.transform(fruits_2d)</span><br><span class="line"><span class="built_in">print</span>(fruits_2d.shape)</span><br><span class="line"><span class="comment"># (300, 10000)</span></span><br><span class="line"><span class="built_in">print</span>(fruits_pca.shape)</span><br><span class="line"><span class="comment"># (300, 50)</span></span><br></pre></td></tr></table></figure>
<br>
<p>10000개 였던 원본 이미지를 50개로 줄였습니다. 이 50개의 feature를 추정하여 복원하는 기능도 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fruits_inverse = pca.inverse_transform(fruits_pca)</span><br><span class="line"><span class="built_in">print</span>(fruits_inverse.shape)</span><br><span class="line"><span class="comment"># (300, 10000)</span></span><br></pre></td></tr></table></figure>
<br>
<p>복원된 차원의 벡터로 그림으로 그려보면 다음과 같습니다.</p>
<table width=80%>
<td width=30%>
    <tr width=400px>
        <img width=300px src=/images/hongong_ml_05_fig7.png>
    </tr>
    <tr>
        <img width=300px src=/images/hongong_ml_05_fig8.png>
    </tr>
    <tr>
        <img width=300px src=/images/hongong_ml_05_fig9.png>
    </tr>
</td>
</table>
<p>조금 흐릿해지고 배경도 어두워졌지만, 확실히 각각의 과일을 복원시켰습니다. 차원을 줄이고, 복원하는걸 보니 주성분 벡터들이 확실히 잘 설명하는 거 같습니다만 아직은 어떤 주성분 벡터가 얼마나 잘 설명하는지 모르겠습니다.</p>
<br>
<p><strong>설명된 분산</strong></p>
<p>주성분이 원본 데이터를 잘 나타내는지 기록한 값을 <strong>설명된 분산</strong><sup>explained variance</sup>이라 합니다.</p>
<p>scikit-learn의 PCA 모델의 explained_variance_ratio_에 50개 주성분에 대한 분산이 저장되어 있습니다. 모두 더하면 원본 데이터에 대한 분산 비율을 얻을 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">exp_var_ratio = pd.DataFrame(pca.explained_variance_ratio_)</span><br><span class="line"><span class="built_in">print</span>(exp_var_ratio.<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># 0    0.921571</span></span><br><span class="line"></span><br><span class="line">exp_var_ratio.plot()</span><br></pre></td></tr></table></figure>
<img width=300px src=/images/hongong_ml_05_fig10.png>
<p>50개의 주성분으로 원본데이터의 92%의 분산을 표현할 수 있습니다.<br />
게다가 첫 번째 주성분 (PC1)으로 데이터의 40% 이상을 표현할 수 있습니다.</p>
<p>이번에는 설명된 분산의 50%에 달하는 주성분을 PCA로 찾아보겠습니다. n_components에 정수가 아닌 실수로 0.5를 입력하면 됩니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components=<span class="number">0.5</span>)</span><br><span class="line">pca.fit(fruits_2d)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pca.n_components_)</span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line">fruits_pca = pca.transform(fruits_2d)</span><br><span class="line"><span class="built_in">print</span>(fruits_pca.shape)</span><br><span class="line"><span class="comment"># (300, 2)</span></span><br></pre></td></tr></table></figure>
<br>
<p>찾아진 주성분은 2개입니다. 위의 그래프를 보시면 아시겠지만 PC1, PC2으로 50% 이상을 표현할 수 있고, 이번에는 n_components에 0.5를 입력하였으므로 PC1, PC2를 찾아준 것입니다.</p>
<p>같은 방식으로 transform을 이용해 원본 데이터를 2개의 차원으로 축소 시켰습니다.</p>
<p>이번에는 <strong>LogisticRegression</strong>으로 분류를 해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_validate</span><br><span class="line"></span><br><span class="line">target = [<span class="number">0</span>]*<span class="number">100</span> + [<span class="number">1</span>]*<span class="number">100</span> + [<span class="number">2</span>]*<span class="number">100</span></span><br><span class="line">lr = LogisticRegression(random_state = <span class="number">42</span>)</span><br><span class="line">scores = cross_validate(lr, fruits_pca, target)</span><br><span class="line"><span class="built_in">print</span>(np.mean(scores[<span class="string">&quot;test_score&quot;</span>]))</span><br><span class="line"><span class="comment"># 0.9933333333333334</span></span><br><span class="line"><span class="built_in">print</span>(np.mean(scores[<span class="string">&quot;fit_time&quot;</span>]))</span><br><span class="line"><span class="comment"># 0.033308219909667966</span></span><br></pre></td></tr></table></figure>
<br>
<p>PC1, PC2만으로도 0.99의 정확도를 내었습니다.</p>
<br>
<p>이번엔 PC1, PC2으로 K-means clustering을 해보겠습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">km = KMeans(n_clusters = <span class="number">3</span>, random_state = <span class="number">42</span>)</span><br><span class="line">km.fit(fruits_pca)</span><br><span class="line"><span class="built_in">print</span>(np.unique(km.labels_, return_counts=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># (array([0, 1, 2], dtype=int32), array([110,  99,  91]))</span></span><br></pre></td></tr></table></figure>
<br>
<p>원본데이터 전체의 특성을 이용했을 때와 거의 같은 결과입니다.</p>
<p>이번에는 산점도를 그려서 cluster를 어떻게 형성했는지 보겠습니다.</p>
<iframe src="/html/hongong_ml_05_fig11.html" width="100%" height="500px" frameborder="0" loading="lazy" allowfullscreen></iframe>
<p>데이터들이 잘 뭉쳐져 있는것이 확인되는군요.</p>
<p>사과와 파인애플은 cluster가 가까이 있음을 확인할 수 있고, 혼동스러운 부분이 있겠습니다. 이렇게 시각화를 해서 데이터를 관찰하면 데이터 분석의 방향성을 고려할 수 있습니다.</p>
<p><br><br></p>
<p>여기서 중간점검을 해보겠습니다.</p>
<p>어떤 데이터에서 scikit-learn의 PCA를 이용해 주성분을 10개를 얻었을 때, 설명된 분산이 가장 큰 주성분은 몇 번째일까요?</p>
<p>1️⃣첫 번째 주성분<br><br />
2️⃣다섯 번째 주성분<br><br />
3️⃣열 번째 주성분<br><br />
4️⃣알 수 없음</p>
<p><br><br><br />
…<br />
<br><br></p>
<p>정답은 <code>1️⃣첫 번째 주성분</code>입니다. 찾아진 주성분은 설명된 분산이 큰 순서부터 찾고 나열합니다. 그래서 대부분 PCA를 이용할 때, 첫 번째 주성분 (PC1)과 두 번째 주성분 (PC2)을 결과로 제시합니다.</p>
<p><br><br></p>
<p>오늘은 K-means clustering, PCA에 대해 정리해보았습니다.<br>어느덧 책 한권의 마무리가 되어가네요.<br>다음에는 인공신경망에 대해 정리하겠습니다.</p>
<p>읽어주셔서 감사드립니다 👋</p>
<p><br><br><br></p>
<!-- flag of hidden posts -->
      
    </div>
    <div class="article-footer">
      <!-- <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://jonghwanyoon.github.io/2023/02/06/hongong-ml-05/" title="ML | K-means clustering, PCA" target="_blank" rel="external">https://jonghwanyoon.github.io/2023/02/06/hongong-ml-05/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote> -->
<!-- 

<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://jonghwanyoon.github.io/" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/DALLE_whale.webp" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://jonghwanyoon.github.io/" target="_blank"><span class="text-dark">jonghwanyoon</span><small class="ml-1x"></small></a></h3>
        <div></div>
      </div>
    </figure>
  </div>
</div>
 -->

    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      </div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/jonghwanyoon" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.linkedin.com/in/jonghwan-yoon" target="_blank" title="Linkedin" data-toggle=tooltip data-placement=top><i class="icon icon-linkedin"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        &copy; 2024 Jonghwan Yoon
        
        <div class="publishby">
        	<!-- Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>. -->
            Theme by <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
    <script defer>
    var disqus_config = function () {
        
            this.page.url = 'https://jonghwanyoon.github.io/2023/02/06/hongong-ml-05/';
        
        this.page.identifier = 'hongong-ml-05';
    };
    (function() { 
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'wg-yoon' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>






    <script defer type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-140384355-2', 'auto');
ga('send', 'pageview');

</script>



</body>
</html>